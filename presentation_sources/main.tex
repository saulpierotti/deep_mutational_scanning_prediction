\documentclass[10pt, british, luatex]{beamer}
\usetheme{MasterThesis}

\addbibresource{./references.bib}

\title[Single Amino Acid Variants Prediction]{Prediction of the Effect of Single Amino Acid Protein
	Variants\\Using Deep Mutational Scanning Data}
\subtitle[]{University of Bologna --- Master Thesis in Bioinformatics}
\author[Pierotti Saul]{Pierotti Saul}
\advisor[]{%
	\setlength{\tabcolsep}{0pt}%
	\begin{tabular}{llll}
		Internal~ & Advisor:~ & Prof.~ & Pietro Di Lena                       \\
		External~ & Advisor:~ & Prof.~ & Arne Elofsson (Stockholm University)
	\end{tabular}
}
\date{July 19, 2021}

\begin{document}

\begin{frame}[plain]
	\titlepage%
\end{frame}

\begin{frame}
	\frametitle{Single Amino Acid Variants}
	A mutation that replaces exactly one amino acid in a protein
	\vfill%
	\begin{figure}
		\input{tikz/mutation_illustration.tex}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Deep Mutational Scanning}
	A high-throughput technique for obtaining fitness information on a large number of mutations in parallel
	\vfill%
	\begin{figure}
		\input{tikz/dms_workflow.tex}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Predicting the Effect of Mutations}
	\begin{columns}[c]
		\begin{column}{.03\textwidth}
		\end{column}
		\begin{column}{.47\textwidth}
			{%
				\huge%
				Why is it useful?
			}
			\vspace{1em}%
			\begin{itemize}
				\item Targeted medical treatments
				\item Protein engineering
			\end{itemize}
		\end{column}
		\begin{column}{.5\textwidth}
			{%
				\huge%
				Why is it needed?
			}
			\vspace{1em}%
			\begin{itemize}
				\item Experiments are insufficient
				\item Experiments are expensive
			\end{itemize}
		\end{column}
	\end{columns}
	\vfill%
	\begin{columns}[c]
		\begin{column}{.23\textwidth}
		\end{column}
		\begin{column}{.6\textwidth}
			{%
				\huge%
				How can it be done?
			}
			\vspace{1em}%
			\begin{itemize}
				\item Machine learning
				\item Statistical models
			\end{itemize}
		\end{column}
		\begin{column}{.17\textwidth}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Previous Work in the Field}
	\begin{figure}
		\input{tikz/variant_effect_predictors.tex}
	\end{figure}
	\vfill%
	Among these predictors only Envision was trained on deep mutational scanning data
\end{frame}

\begin{frame}
	\frametitle{My Approach}
	\begin{figure}
		\input{tikz/project_structure.tex}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Training Data}
	I borrowed the training dataset of the predictor Envision {\footnotesize\parencite{Gray2018}}
	\vfill%
	% looks better not centered
	{%
		\let\bfseries\sbseries%
		\input{tikz/tikzdevice/presentation_fitness_score_distribution.tex}
		\input{tikz/tikzdevice/presentation_datasets_abundancies.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Experiments Do Not Agree Much with Each Other}
	Two independent deep mutational scanning experiments on Ubiquitin are present in the training dataset.
	Their correlation is low.
	\vfill%
	\begin{center}
		{%
			\let\bfseries\sbseries%
			\input{tikz/tikzdevice/presentation_datasets_correlations.tex}
		}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Interesting Patterns in Mutation Sensitivity}
	\begin{columns}[c]
		\begin{column}{.5\textwidth}
			\vfill\vspace{-1.5em}\null%
			\begin{center}
				{%
					\let\bfseries\sbseries%
					\input{tikz/tikzdevice/presentation_mutation_identity.tex}
				}
			\end{center}
		\end{column}
		\begin{column}{.5\textwidth}
			\vspace{-7.5em}
			\begin{itemize}
				\item Polar residues seem more tolerant to mutations than hydrophobic residues
				\item Proline (P) is the most disruptive residue
				\item Tryptophan (W) is hard to replace
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Exposure Explains the Mutability of Polar Residues}
	When filtering by Relative Solvent Accessibility (RSA) apolar residues are \textbf{\textit{not}} more sensitive to mutations than polar residues
	\vfill%
	\begin{center}
		{%
			\hspace{-2em}
			\let\bfseries\sbseries%
			\input{tikz/tikzdevice/presentation_mutation_identity_exposed.tex}
			\hspace{2em}%
			\input{tikz/tikzdevice/presentation_mutation_identity_buried.tex}
		}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Buried Residues Are More Conserved}
	\vfill%
	% looks better not centered
	{%
		\let\bfseries\sbseries%
		\input{tikz/tikzdevice/presentation_score_dssp_rsa.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Features Used by the Predictors}
	All the features are derived from the sequences: I did \textbf{\textit{not}} use any structural information
	\vfill%
	\begin{figure}
		\input{tikz/features.tex}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Single Protein Models}
	\begin{columns}[c]
		\begin{column}{.6\textwidth}
			\vfill\null%
			{%
				\centering%
				\let\bfseries\sbseries%
				\input{tikz/tikzdevice/presentation_single_protein_models_performance_comparison.tex}
			}
		\end{column}
		\begin{column}{.4\textwidth}
			Naive approach\vspace{2ex}
			\begin{itemize}
				\item A different model trained for each protein
				\item Half of the mutations used for testing and half for cross-validation
				\item Too good to be true
			\end{itemize}
			\vfill\null%
			\visible<2->{%
				Segregating protein positions\vspace{2ex}
				\begin{itemize}
					\item Same as above but mutations in the same position segregated in the training or testing sets
					\item Performances are more realistic
				\end{itemize}
			}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Leave-One-Protein-Out (LOPO) Models}
	\begin{columns}[c]
		\begin{column}{.6\textwidth}
			\vfill\null%
			{%
				\centering%
				\let\bfseries\sbseries%
				\input{tikz/tikzdevice/presentation_general_models_performance_comparison.tex}
			}
		\end{column}
		\begin{column}{.4\textwidth}
			\vspace{-3em}
			\begin{itemize}
				\item Models trained on the whole dataset while leaving one protein out
				\item For the left-out protein, half of the mutations used for testing and half for validation
				\item Spearman correlation coefficient used for evaluation
			\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Discussion and Future Directions}
	{%
		\Large%
		What I learned
	}
	\vspace{1ex}
	\begin{itemize}
		\item The testing strategy is crucial
		\item Good performances without structural features
		\item Strong variability between datasets
		\item Complex models not necessarily better
	\end{itemize}
	\vfill%
	{%
		\Large%
		Ideas for the future
	}
	\vspace{1ex}
	\begin{itemize}
		\item Using residue contacts in a graph convolutional neural network
		\item Training on more deep mutational scanning studies
		\item Finding a better normalization strategy
	\end{itemize}
\end{frame}

\appendix%
\begin{frame}
	\frametitle{\  }
	\Huge\centering Questions?
\end{frame}

\begin{frame}[allowframebreaks]
	\frametitle{Bibliography}
	\printbibliography{}%
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Secondary structure is of limited importance in the discrimination of damaging mutations\\[1ex]
	{%
	\centering%
	\let\bfseries\sbseries%
	\input{tikz/tikzdevice/presentation_score_dssp_q3.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	The correlation among features follows predictable patterns\\[1ex]
	{%
	\centering%
	\let\bfseries\sbseries%
	\input{tikz/tikzdevice/presentation_feature_corr_heatmap.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Precision of trRosetta {\footnotesize\parencite{Yang2020}} in predicting residue contacts

	\tiny%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lrrrrrr}%
		\toprule
		& \multicolumn{3}{l}{Medium-range ($s \geq 12$)} & \multicolumn{3}{l}{Long-range ($s \geq 24$)} \\
		\cmidrule(l){2-7}
		Dataset & Top $L/5$ & Top $L/2$ & Top $L$ & Top $L/5$ & Top $L/2$ & Top $L$ \\
		\midrule
		beta-lactamase & \num{1.0000000} & \num{0.9152542} & \num{0.8644068} & \num{0.9565217} & \num{0.9322034} & \num{0.7627119} \\
		WW\_domain     & \num{0.9523810} & \num{0.8962264} & \num{0.8262911} & \num{0.9047619} & \num{0.8679245} & \num{0.7464789} \\
		PSD95pdz3      & \num{0.9607843} & \num{0.9212598} & \num{0.7960784} & \num{0.9215686} & \num{0.8110236} & \num{0.6980392} \\
		kka2\_1:2      & \num{1.0000000} & \num{1.0000000} & \num{0.9594595} & \num{1.0000000} & \num{1.0000000} & \num{0.8918919} \\
		hsp90          & \num{1.0000000} & \num{1.0000000} & \num{0.9594595} & \num{1.0000000} & \num{1.0000000} & \num{0.8918919} \\
		Ubiquitin      & \num{0.9807692} & \num{0.9160305} & \num{0.8212928} & \num{1.0000000} & \num{0.9007634} & \num{0.7034221} \\
		Pab1           & \num{0.8000000} & \num{0.7179487} & \num{0.6666667} & \num{0.8666667} & \num{0.7435897} & \num{0.6025641} \\
		E1\_Ubiquitin  & \num{0.8181818} & \num{0.8571429} & \num{0.7719298} & \num{0.9090909} & \num{0.7500000} & \num{0.5438596} \\
		gb1            & \num{1.0000000} & \num{0.8500000} & \num{0.4634146} & \num{0.6250000} & \num{0.4000000} & \num{0.2195122} \\
		\bottomrule
	\end{tabular*}%
	\vfill%

	\normalsize Quality of the predicted structural features from NetsurfP-2 {\footnotesize\parencite{Klausen2019}}

	\tiny%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llr}%
		\toprule
		Feature                        & Evaluation metric      & Score                     \\
		\midrule
		Relative solvent accessibility & Pearson $r$            & \num{0.7882348}           \\
		Accessible surface area        & Pearson $r$            & \num{0.7957144}           \\
		$Q_3$ secondary structure      & $Q_3$ accuracy         & \num{0.8505}              \\
		$Q_8$ secondary structure      & $Q_8$ accuracy         & \num{0.7183}              \\
		$\phi$ torsion angle           & Circular correlation   & \num{0.734590829057844}   \\
		$\psi$ torsion angle           & Circular correlation   & \num{0.8681192893807274}  \\
		\bottomrule
	\end{tabular*}%
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Relationship between the features used in the models and the fitness scores

	\tiny%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lrrr}%
		\toprule
		Feature                                                  & Pearson $r$      & Spearman $\rho$  & Kendall $\tau$    \\
		\midrule
		PSSM mutation score                                      & \num{-0.2869455} & \num{-0.2809048} & \num{-0.1885174}  \\
		Netsurf predicted RSA                                    & \num{0.3436087}  & \num{0.3729762}  & \num{0.2515731}   \\
		Netsurf predicted ASA                                    & \num{0.3190383}  & \num{0.3541994}  & \num{0.2384785}   \\
		Netsurf predicted disorder                               & \num{0.05817099} & \num{0.1760797}  & \num{0.1183413}   \\
		EVcouplings epistatic model                              & \num{0.462962}   & \num{0.495414}   & \num{0.3427753}   \\
		EVcouplings independent model                            & \num{0.4388572}  & \num{0.4364188}  & \num{0.300382}    \\
		EVcouplings frequency                                    & \num{0.1864407}  & \num{0.3472157}  & \num{0.2400341}   \\
		EVcouplings conservation                                 & \num{-0.3159081} & \num{-0.3329435} & \num{-0.2264373}  \\
		Closeness centrality (trRosetta predicted contacts)      & \num{-0.1630601} & \num{-0.1681213} & \num{-0.112927}   \\
		Betweenness centrality (trRosetta predicted contacts)    & \num{-0.2004047} & \num{-0.2902508} & \num{-0.1949691}  \\
		Degree centrality (trRosetta predicted contacts)         & \num{-0.1230379} & \num{-0.1346042} & \num{-0.08988814} \\
		Load centrality (trRosetta predicted contacts)           & \num{-0.1979317} & \num{-0.2877618} & \num{-0.193219}   \\
		Harmonic centrality (trRosetta predicted contacts)       & \num{-0.1907764} & \num{-0.2000805} & \num{-0.1373126}  \\
		Clustering coefficient (trRosetta predicted contacts)    & \num{0.2309697}  & \num{0.2533754}  & \num{0.1721249}   \\
		\midrule
	\end{tabular*}\\%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lr}%
		& Linear-circular correlation \\
		\midrule
		Netsurf predicted $\phi$ torsion angle  & \num{0.01213396} \\
		Netsurf predicted $\psi$ torsion angle  & \num{0.01673723} \\
		\midrule
	\end{tabular*}\\%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lrr}%
		& Kruskal-Wallis $\chi^2$ & $p$-value           \\
		\midrule
		Wild-type residue                        & \num{1482.4}            & $<$ \num{2.2e-16} \\
		Mutated residue                          & \num{708.53}            & $<$ \num{2.2e-16} \\
		Netsurf predicted $Q_3$ secondary structure & \num{215.33}            & $<$ \num{2.2e-16} \\
		Netsurf predicted $Q_8$ secondary structure & \num{351.97}            & $<$ \num{2.2e-16} \\
		\bottomrule
	\end{tabular*}%
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Feature importances for the gradient boosted tree general models\\[1ex]
	{%
	\centering%
	\let\bfseries\sbseries%
	\input{tikz/tikzdevice/presentation_lopo_models_xgb_feature_importance_by_dataset.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Feature importances for the linear regression general models\\[1ex]
	{%
	\centering%
	\let\bfseries\sbseries%
	\input{tikz/tikzdevice/presentation_lopo_models_linear_feature_importance_by_dataset.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Validation and testing performances for the single protein models trained with the naive approach\\[1ex]
	{%
	\centering%
	\let\bfseries\sbseries%
	\input{tikz/tikzdevice/presentation_single_protein_models_naive_test_result_summary.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Validation and testing performances for the single protein models trained by segregating protein positions\\[1ex]
	{%
	\centering%
	\let\bfseries\sbseries%
	\input{tikz/tikzdevice/presentation_single_protein_models_by_position_test_result_summary.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Comparison of the performances of single protein models and general models\\[1ex]
	\vspace{\baselineskip}%
	{%
		\centering%
		\let\bfseries\sbseries%
		\input{tikz/tikzdevice/presentation_xgb_models_performance_comparison.tex}
	}
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Confidence intervals in prediction performances

	\tiny%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llll}%
		\toprule
		Dataset name            & Model       & \num{95}\,\% C.I. (Pearson) & \num{95}\,\% C.I. (Spearman) \\
		\midrule
		beta-lactamase & Naive       & \numrange{0.8866}{0.9077}        & \numrange{0.8578}{0.8803}         \\
		beta-lactamase & By position & \numrange{0.7927}{0.8272}        & \numrange{0.7461}{0.7820}         \\
		beta-lactamase & LOPO        & ---                              & \numrange{0.6781}{0.7210}         \\
		WW\_domain     & Naive       & \numrange{0.6747}{0.8174}        & \numrange{0.7044}{0.8368}         \\
		WW\_domain     & By position & \numrange{0.5679}{0.7291}        & \numrange{0.5203}{0.7168}         \\
		WW\_domain     & LOPO        & ---                              & \numrange{0.5628}{0.7297}         \\
		PSD95pdz3      & Naive       & \numrange{0.7443}{0.8313}        & \numrange{0.7276}{0.8008}         \\
		PSD95pdz3      & By position & \numrange{0.5487}{0.6652}        & \numrange{0.5442}{0.6485}         \\
		PSD95pdz3      & LOPO        & ---                              & \numrange{0.5335}{0.6282}         \\
		kka2\_1:2      & Naive       & \numrange{0.7405}{0.7763}        & \numrange{0.7232}{0.7588}         \\
		kka2\_1:2      & By position & \numrange{0.6451}{0.6881}        & \numrange{0.6401}{0.6824}         \\
		kka2\_1:2      & LOPO        & ---                              & \numrange{0.5975}{0.6427}         \\
		hsp90          & Naive       & \numrange{0.8190}{0.8664}        & \numrange{0.6522}{0.7071}         \\
		hsp90          & By position & \numrange{0.6945}{0.7577}        & \numrange{0.4996}{0.5674}         \\
		hsp90          & LOPO        & ---                              & \numrange{0.3771}{0.4529}         \\
		Ubiquitin      & Naive       & \numrange{0.7470}{0.8281}        & \numrange{0.7782}{0.8321}         \\
		Ubiquitin      & By position & \numrange{0.4858}{0.6035}        & \numrange{0.4565}{0.5672}         \\
		Ubiquitin      & LOPO        & ---                              & \numrange{0.2951}{0.4296}         \\
		Pab1           & Naive       & \numrange{0.8047}{0.8749}        & \numrange{0.7482}{0.8239}         \\
		Pab1           & By position & \numrange{0.6545}{0.7590}        & \numrange{0.6423}{0.7288}         \\
		Pab1           & LOPO        & ---                              & \numrange{0.6038}{0.7004}         \\
		E1\_Ubiquitin  & Naive       & \numrange{0.7454}{0.8470}        & \numrange{0.6487}{0.7517}         \\
		E1\_Ubiquitin  & By position & \numrange{0.5898}{0.7218}        & \numrange{0.3623}{0.5100}         \\
		E1\_Ubiquitin  & LOPO        & ---                              & \numrange{0.5027}{0.6343}         \\
		gb1            & Naive       & \numrange{0.8965}{0.9341}        & \numrange{0.8759}{0.9194}         \\
		gb1            & By position & \numrange{0.3072}{0.4561}        & \numrange{0.2942}{0.4522}         \\
		gb1            & LOPO        & ---                              & \numrange{0.3225}{0.4706}         \\
		\bottomrule
	\end{tabular*}%
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Statistical significance of performance differences. Starred values are significant with Bonferroni correction.

	\begin{equation*}
		\alpha = \frac{0.05}{27} = 0.00185185
	\end{equation*}

	\tiny%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lllll}%
		\toprule
		Dataset name            & Model 1           & Model 2                & $p$-value      \\
		\midrule
		beta-lactamase & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		beta-lactamase & Linear regression & EVmutation             & \num{1e-4} * \\
		beta-lactamase & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		WW\_domain     & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		WW\_domain     & Linear regression & EVmutation             & \num{1e-4} * \\
		WW\_domain     & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		PSD95pdz3      & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		PSD95pdz3      & Linear regression & EVmutation             & \num{1e-4} * \\
		PSD95pdz3      & EVmutation        & Gradient boosted trees & \num{0.514}  \\
		kka2\_1:2      & Linear regression & Gradient boosted trees & \num{0.0043} \\
		kka2\_1:2      & Linear regression & EVmutation             & \num{0.0053} \\
		kka2\_1:2      & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		hsp90          & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		hsp90          & Linear regression & EVmutation             & \num{1e-4} * \\
		hsp90          & EVmutation        & Gradient boosted trees & \num{0.2358} \\
		\bottomrule
	\end{tabular*}%
	\normalsize%

	The table continues on the next slide
\end{frame}

\begin{frame}
	\frametitle{Supplementary Material}
	Statistical significance of performance differences. Starred values are significant with Bonferroni correction.

	\begin{equation*}
		\alpha = \frac{0.05}{27} = 0.00185185
	\end{equation*}

	The table continues from the previous slide

	\tiny%
	\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lllll}%
		\toprule
		Dataset name            & Model 1           & Model 2                & $p$-value      \\
		\midrule
		Ubiquitin      & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		Ubiquitin      & Linear regression & EVmutation             & \num{1e-4} * \\
		Ubiquitin      & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		Pab1           & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		Pab1           & Linear regression & EVmutation             & \num{1e-4} * \\
		Pab1           & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		E1\_Ubiquitin  & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		E1\_Ubiquitin  & Linear regression & EVmutation             & \num{1e-4} * \\
		E1\_Ubiquitin  & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		gb1            & Linear regression & Gradient boosted trees & \num{1e-4} * \\
		gb1            & Linear regression & EVmutation             & \num{1e-4} * \\
		gb1            & EVmutation        & Gradient boosted trees & \num{1e-4} * \\
		\bottomrule
	\end{tabular*}%
\end{frame}

\end{document}
