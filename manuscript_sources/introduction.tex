\thispagestyle{empty}
\cleardoublepage%
\chapter{Introduction}
Predicting the effect of mutations is of fundamental importance for a wealth of biological problems.
The goal of precision medicine of providing targeted medical treatment \parencite{Goetz2018} requires an in-depth understanding of the effect of genomic differences, and current experimental assays are not yet capable of providing this information with high enough throughput \parencite{Reeb2020}.
This is why computational tools that can fill the void of uncharacterized mutations are of extreme interest.
Traditionally these tools have been trained on manually annotated protein variants, incorporating the bias which is inherent in such a subjective dataset \parencite{Gray2018}.
The emergence of next-generation sequencing coupled with functional selection assays made it possible to assess the effect of mutations in a more unbiased fashion.
This technique, which also offers a much higher throughput compared to previous approaches, is called deep mutational scanning \parencite{Fowler2014}.
Even though deep mutational scanning is not yet capable of solving the problem of characterizing most of the human genotypic variation, not even for the coding genome, it can provide valuable training data for computational tools.
This work aims at taking advantage of deep mutational scanning data for training a family of predictors of the effect of single amino acid variants.
Core advancements in variant effect prediction will be explored in the next sections, together with a description of the biological significance of mutations, experimental methods for obtaining them, and a high-level overview of the machine learning approaches used.

It is worth pointing out that, even though variant effect prediction has potential uses in human medicine, the focus of this work is not on the clinical applications.
No implicit claim that the predicted scores can be interpreted as a proxy for the pathogenicity of a variation is made.
Being the models trained on deep mutational scanning data, the predictions should be only interpreted as an approximation of deep mutational scanning fitness.
This may or may not be correlated to pathogenicity, depending on the specific experimental selection strategy used.

\section{Proteins}
A protein is a linear chain of amino acid residues linked together by amidic bonds, which folds in a three-dimensional shape as a virtue of the interactions among residue side chains, backbone atoms, solvent molecules, and other components of biological organisms (membranes, other proteins, ions, small molecules).
Proteins can be classified as soluble if they are free-floating in the cytosol or the extracellular environment, or membrane-bound if they are inserted in a biological membrane (the cell membrane or the membrane of an organelle or a vesicle).
An intermediate category is peripheral membrane proteins, which are linked to a biological membrane through lipid anchors, or weak interactions with other integral membrane proteins.

Amino acids are small molecules presenting a central carbon atom, called $C_\alpha$, bound to a carboxylic group, an amino group, a hydrogen atom, and a side chain.
Sidechains can be of \num{20} different types (plus some additional non-standard configurations), and the identity of the side chain determines the identity of the amino acid.
In proteins, the carboxylic group of an amino acid forms an amidic (also called peptidic) bond with the amino group of another amino acid, forming a chain of connected amino acid which constitutes the protein.
When integrated into a protein chain, an amino acid does not present any more the carboxylic and amino groups, which are now embedded in amidic bonds, but only the residual side chain is distinguishable.
For this reason, amino acids embedded in a protein chain are called residues.
The linear chain of repeating amino groups, carboxylic groups, and $C_\alpha$ atoms forms the backbone of the protein.
The torsion angles among backbone atoms define the three-dimensional configuration of the protein.
In general, the bond between the nitrogen of the amino group and the $C_\alpha$, and the bond between the $C_\alpha$ and the carboxylic carbon are free to rotate (defining torsion angles called $\phi$ and $\psi$, respectively), while the peptidic bond itself is rigid and planar because of resonance with the carboxylic oxygen.
Alternative peptidic bond torsion angle configurations (called $\omega$), however, are observed with proline.

The \num{20} standard residues can be categorized, according to the chemical property of their side chains, in apolar aliphatic, apolar aromatic, polar, positively charged and negatively charged.
Apolar aliphatic residues are alanine (A), leucine (L), isoleucine (I), and methionine (M).
Methionine is different from other aliphatic residues in that it presents a sulfur atom in a thioether bond along the side chain.
Apolar aromatic residues are tyrosine (Y), phenylalanine (F), and tryptophan (W).
Polar residues are serine (S), threonine (T), asparagine (N), glutamine (Q), and cysteine (C).
Positively charged residues are lysine (K), histidine (H), and arginine (R).
Arginine can, however, also effectively interact with aromatic residues thanks to the highly delocalised electronic configuration of its guanidino group.
Negatively charged residues are aspartic acid (D), and glutamic acid (E).
Proline (P), glycine (G), and cysteine (C) are a bit harder to categorize.
Proline is the only amino acid to present a covalent bond between the side chain and a backbone atom different from $C_\alpha$ (it forms a bond with the aminic nitrogen).
Glycine does not have a side chain at all, being replaced by a single hydrogen atom.
Cysteine presents a terminal thiol group on its side chain, and it can form covalent bonds with other cysteine residues (disulphide bridges) in oxidising conditions (particularly extracellularly).

The three-dimensional shape of a protein is responsible for its function and it is conceptualized in a hierarchy of interactions.
The sole determinant of the three-dimensional shape of a protein in its native conformation is the identity and order of the residues composing it.
The primary structure of a protein is the linear chain of residues, held together by covalent bonds.
The secondary structure refers to stereotypical modes of interaction of the protein backbone, which gives rise to a limited number of well-distinguishable structural patterns: helices, strands, turns, and coils \parencite{Kabsch1983}.
The secondary structure of a residue is fully described by its backbone torsion angles \parencite{Ramachandran1963}.
The identity of the residues composing the primary structure of a protein determines which backbone torsion angles are favoured, and thus which secondary structures will be formed.
The interactions among secondary structure elements, usually mediated by side-chain atoms, are responsible for the tertiary structure of a protein.
The tertiary structure of a protein is its complete three-dimensional shape and it is responsible for protein function since it determines the interactions that the protein will be able to have with other molecules.
Some proteins can interact in a stable manner and form complexes.
The mode of interaction among protein subunits represents the protein quaternary structure.

The interactions responsible for protein structure, besides the covalent bonds responsible for the primary sequence, are almost all weak interactions.
Hydrogen bonds have a primary role, especially among backbone atoms in the establishment of secondary structures (but they can also be present among side-chain atoms among polar and charged residues).
Electrostatic interactions among charged side-chains are also important.
Apolar residues experience hydrophobic forces when in a polar solvent.
London dispersion forces among hydrophobic residues are also present.
Disulphide bridges are among the few known covalent interactions responsible for higher protein organization, and they are exclusive of cysteine residues.
Other more sporadic covalent interactions have also been observed \parencite[][for example]{Ito1991}.
Very recently, \textcite{Wensien2021} reported the observation of a new type of covalent cross-link in proteins between lysine and cysteine, consisting of a NOS bridge.

In general, soluble globular proteins consist of a hydrophobic core, which is essential for the stability of the fold, and a surface that interfaces with the solvent.
Interestingly, the presence of this hydrophobic core seems to be a universal feature of protein domains \parencite{Kalinowska2017}.

\section{Mutations and mutagenesis}
A mutation is any alteration in the sequence of a genome or an isolated nucleotide fragment.

Broadly, it is possible to distinguish mutations that affect single nucleotides (also called point mutations) and mutations that involve the insertion, deletion, or re-arrangement of larger portions of the genome.
Insertions, deletions and re-arrangements have a profound effect on the protein product of the affected gene, but they are not the focus of this work.

In the context of protein-coding genes, a point mutation can be classified according to its effect at the residue level.
Synonymous mutations alter the nucleotide sequence of the gene but leave the amino acid sequence of the protein unchanged.
This is possible since the genetic code is degenerate and different codons can code for the same residue.
This is not to say, however that synonymous mutations do not have any physiological effect: different codons for the same residue can be translated at different rates, affecting the amount of protein produced, and the nucleotide difference can affect splicing and the binding of proteins to the gene's DNA and mRNA\@ \parencite{Hunt2014}.

Non-synonymous mutations, on the contrary, affect the residue composition of the protein product.
They do so by altering the wild-type codon to a codon that codes for a different residue type.
The effect of non-synonymous mutations includes all the nucleotide-level alterations discussed for synonymous mutations, but it adds also the effect that the residue substitution has on the protein product.
Substituting a residue in a protein can affect protein folding, stability, and function, aspects explored more in detail in \autoref{sec:SAV}.
Because of their direct and causal effect on protein function, non-synonymous mutations on protein-coding genes, also called Single Aminoacid Variants (SAVs), are the main focus of this work.

The third category of point mutations in protein-coding genes is that of nonsense mutations.
These mutations involve the conversion of a residue-coding codon (also called sense codon) to a stop codon (nonsense codon).
The effect of nonsense mutations is often dramatic since they cause the premature termination of protein synthesis, causing the elimination of potentially vital portions of the protein.

Even though mutations happen spontaneously and they are one of the driving forces of evolution, they can also be caused willingly by researchers.
This process is termed mutagenesis and involves experimentally generating genetic variants of a given sequence.
When applied to protein-coding genes, mutagenesis can be invaluable in shedding light on the catalytic mechanism of enzymes \parencite{Peracchi2001}, on the mechanics of protein folding and the residue interactions underpinning protein stability \parencite{Nisthal2019}.
Mutagenesis has also clear potentials for livestock and crops improvement and in genetic engineering \parencite{Davies1988, Holme2019, Kalds2019}.

\subsection{Effect of residue substitutions in proteins}\label{sec:SAV}

The effect of substituting a residue for another in a protein can vary dramatically according to the identity of the introduced residue, the local biophysical environment, and the mechanistic function of the protein.

Missense mutations in proteins typically result in large perturbations of stability and can lead to aggregation propensity \parencite{DePristo2005}.
The effect of a mutation on the stability of a protein can be measured in terms of the difference in folding free energy change ($\Delta G)$ between the wild-type and mutated protein (difference called $\Delta\Delta G$).
Multiple mutations have an approximatively additive effect on stability, even though epistatic effects with a small number of other positions are common \parencite{Daopin1991, Green1993}.
Interestingly, most $\Delta\Delta G$ values are on the same order of magnitude as $\Delta G$ values \parencite{DePristo2005}.
Destabilizing mutations also result in increased aggregation, owing to the larger pool of unfolded proteins that they cause \parencite{Chiti2000,RamirezAlvarado2000}.
Mutations that influence aggregation independently from stability are also known \parencite{Mitraki1991,Pawar2005}.
Mutations that affect directly function are rarer since they need to occur on catalytic residues \parencite{DePristo2005}.


\subsection{Low- and medium-throughput mutagenesis}
The first attempts at genetically modifying a biological sequence were done in a non-targeted fashion, by exposing cells to a mutagenic agent such as ionizing radiations and then selecting among the many random mutants generated those deemed of interest on a phenotypic standpoint \parencite{Muller1927, Stadler1928}.

Site-directed mutagenesis \parencite[reviewed in][]{Shortle1981} was developed in the 1970s' by Clyde Hutchison, Marshall Edgell, and Michael Smith and paved the way for a wealth of experimental techniques aimed at precisely manipulating biological sequences.
The development of these methods later earned the 1993 Nobel prize in Chemistry to Michael Smith, a prize shared with the inventor of the Polymerase Chain Reaction (PCR) Kary B. Mullis.

The site-directed introduction of point mutations can be achieved by annealing a mutant DNA oligomer to the wild-type sequence to be mutated and using it as a primer to perform elongation using DNA polymerase \parencite{Hutchison1978}.
The resulting mutant sequence can be then amplified by PCR and inserted in the organism of interest, obtaining the respective protein product and examining the phenotypic effect of the mutation.
A more advanced technique to sample the mutational landscape of a protein applies site-directed mutagenesis repeatedly to a stretch of biological sequence, examining the effect of mutations at different positions.
This technique, called single aminoacid scanning mutagenesis, usually performs substitutions towards a single residue and typically features alanine \parencite[reviewed in][]{Morrison2001}.
Alanine is an ideal candidate for such studies since, compared to other residues and thanks to its simple methyl sidechain, it does show the effect of not having the wild-type functional groups in a given position while also it does not dramatically alter backbone flexibility.

\subsection{Deep mutational scanning}
Even though site-directed mutagenesis and alanine scanning greatly served the genetics and biophysics communities, recent developments in sequencing technologies made it possible to extend the technique even further.

Deep mutational scanning \parencite{Fowler2010, Fowler2014} is a high-throughput method for the characterization of large numbers of mutations in a given protein sequence.
When all, or approximately all, the single amino acid variants of a sequence are produced and examined, the technique also takes the name of saturation mutagenesis.

The typical deep mutational scanning workflow involves the generation of a library of mutants, a functional assay for the selective enrichment of mutants that affect positively a property of interest, and a next-generation sequencing step to precisely quantify this selective effect.

The mutant libraries can be randomly produced or can contain a methodical selection of all the single residue mutations, all the double residue mutations, or higher-order combinations.
The choice of the library is tightly coupled to the scientific question to be addressed.
For example, double mutant libraries are essential in the study of pairwise residue interactions \parencite[as in][]{Olson2014}, while single mutant libraries are much easier to produce and assay, and are sufficient for the study of the mutational sensitivity of specific positions.

The first deep mutational scan used site-directed mutagenesis with doped oligomers \parencite{Knight2003} to randomly generate mutants for the WW domain of the protein Yap65 \parencite{Fowler2010}.
This library generation approach makes it almost impossible to obtain all the single mutants for a sequence, given its bias against multiple nucleotide substitutions.
Site-saturation mutagenesis \parencite{Jain2014} is more apt at generating complete single mutant libraries.
Since this technique requires an individual PCR reaction for each position to be mutagenized, however, it is quite time-consuming.

The functional assay is a fundamental part of any deep mutational scanning experiment, and the choice of which kind of assay to use is inevitably coupled to that of the expression vector and to the scientific question that motivated the experiment.
More or less directly, protein stability is frequently one of the selected properties.
For example, in binding assays, where antibodies or natural binding partners are used to selectively bind the protein of interest, the shape of the protein is of central importance for the selection, and this in turns depends on its thermodynamic stability.

Fluorescent or fluorescently-labelled proteins or antibodies can be used together with a Fluorescence-Activated Cell Sorter (FACS) for performing \textit{in vitro} variant selection.
Deep mutational scanning experiments that take advantage of a fluorecence-based \textit{in vitro} selection step include \textcite{Sarkisyan2016,Bhagavatula2017,Matreyek2017}.

Survival and competition assays performed with the mutant protein transfected in live cells can give a more complete picture of the effect of a given mutation in the organism, but at the same time the results will be confounded by the many ways that a protein can influence organismal fitness, and it can be harder to pinpoint a certain effect to precise biophysical changes in protein structure.
These kinds of growth assays were, for example, used in \textcite{Starita2015, Mishra2016}.

Next-generation sequencing is a central part of the deep mutational scanning pipeline.
Short-read and high throughput technologies such as Illumina \parencite{Bennett2004} are commonly used.
A shortcoming of Illumina sequencing is the short read length, which makes it impossible to sequence through a typical human gene in one pass.
This is particularly problematic in deep mutational scanning since many almost identical sequences (mutants) are sequenced together and must be told apart from each other.
The typical solution to this problem is the use of DNA barcoding \parencite{Hiatt2010}.
With DNA barcoding, short (\SI{< 20}{bp}) random sequences are cloned next to the mutagenized regions and are sequenced and quantified before and after selection in place of the actual mutations.
Subsequently, a sequencing platform capable of longer reads such as PacBio \parencite{Eid2009} is used for associating each barcode to a specific mutation.

The scoring of the fitness of a variant in a deep mutational scanning experiment can be done in several ways.
When only two timepoints of variant frequencies are present (i.e.\ before and after a selection step), the logarithmic ratio of the two frequencies can be used as a fitness score.
When more time points are present, a linear regression model can be applied between mutant frequency and time, and the slope of the regression line will represent the fitness score.

The statistical model Enrich2 \parencite{Rubin2017} offers automation and standardization in the determination of fitness scores from raw sequencing data from deep mutational scanning experiments.

\section{Variant effect prediction}
Despite the potentials from deep mutational scanning, the goal of having a complete mutational effect atlas even just for the human proteome is still not in reach.
For model organisms, this is a target that is not even in sight.
Nonetheless, assessing the effect of mutations is a central theme in personalised medicine \parencite{Goetz2018}.

The need for phenotypically scoring variants and the limitations of the available experimental techniques prompted the development of computational tools aimed at predicting the effect of unseen variants.
The first generation of variant effect predictors was created to characterize the vast amount of novel genetic variants discovered with the advent of genome sequencing projects \parencite{Collins1998,Ng2001,Ramensky2002}.
These predictors were developed as classifiers, that produced a binary classification of mutations as pathogenic/non-pathogenic.

Some of the variant effect predictors that have been developed are simple statistical models, while others take advantage of more complex machine learning algorithms.
Sorting Intolerant from Tolerant \parencite[SIFT,][]{Sim2012} and SIFT for genomes \parencite[SIFT 4G,][]{Vaser2015} use the likelihood of observing a given residue in a specific position in a multiple sequence alignment of homologous sequences as a proxy for assessing pathogenicity.
They then produce binary predictions by imposing a threshold on the likelihood score.
PolyPhen-2 \parencite{Adzhubei2010} is another binary variant effect predictor that uses sequence-based and structure-based predictive features with a naive Bayes classifier.
Evolutionary Action \parencite{Katsonis2014} is also a statistical method, similarly to SIFT\@.
It was trained exclusively on human Single Nucleotide Polymorphisms (SNPs), and thus it is optimized to predict only the effect of human variants.
SNAP2 \parencite{Hecht2015} is an ensemble neural network-based classifier that uses structural features, evolutionary features, and biophysical features of the mutant and wild-type residues.
In addition, this method uses a sliding window around the mutated position to obtain information on the sequence context of the mutation.
Combined Annotation Dependent Depletion \parencite[CADD,][]{Rentzsch2021,Rentzsch2018,Kircher2014} is a predictor that in addition to predicting the effect of single amino acid variants, it also predicts the effect of insertions, deletions, nonsense mutations, and mutations that alter splicing.
It was first implemented as a support vector machine in \parencite{Kircher2014}, and then as a logistic regression in \parencite{Rentzsch2018}.
The peculiarity of CADD is that instead of relying on annotated pathogenic variants, it is trained in distinguishing mutations that became fixed in the human lineage after the split with the last human-ape ancestor (termed proxy-neutral mutations) from a simulated set of \textit{de novo} variants (termed proxy-deleterious).
Envision \parencite{Gray2018} is a more recent, quantitative predictor that was directly trained on deep mutational scanning data.
It features a gradient-boosting tree algorithm and it uses a combination of structural, evolutionary, and sequence features.
Unsupervised models such as EVmutation \parencite{Hopf2017} and DeepSequence \parencite{Riesselman2018} have also been developed and perform surprisingly well.
EVmutation is a statistical model that evaluates the epistatic effect between protein positions by examining multiple sequence alignments, but without ever being exposed to experimental variant effect scores.
DeepSequence has a similar conception to that of EVmutation, but it models higher-order dependencies between protein positions by using a deep variational autoencoder.

The performances of variant effect predictors, especially in regression, is currently quite poor.
It was observed that a simple position-specific scoring matrix can perform better than complex, machine learning-based predictors \parencite{Reeb2020}.
Poor performances, especially in the case of neutral mutations at evolutionarily conserved positions, could be due to a bias in the data used in training \parencite{Liu2013}.

\section{Machine learning}
Machine learning is a central tool in bioinformatics when the prediction of some quantity is of interest.
Here I will briefly summarise the rationale behind the machine learning approaches used in this work.
Instead of going into the mathematical details of the implementations used, which can, in any case, be explored by the interested reader in the respective original publications, this section wants to give an intuitive overview of the methods and practices that were used in the development of this project.
In particular, an explanation of the effect of the hyperparameters optimised in this work is provided.
For a more detailed overview of machine learning and its uses in computational biology, the reader can refer to \textcite{Chicco2017}.

\subsection{Overfitting}
Overfitting is the failure of a trained model in generalizing to data not seen during training and optimization.

Overfitting is a typical problem that affects overparameterized models, where the complexity of the architecture is not justified by complexity in the relationship between independent and dependent variables.
In extreme cases, a model will just memorize the training instances, without actually learning the relationship of interest between the features and the target variable.
The opposite problem to overfitting is underfitting, which happens when a model is too simple to capture the complexity in the data.

To reduce overfitting is possible, depending on the circumstances, to choose a simpler model, increase the amount of data, or apply regularization.
A good discussion of overfitting is available in \textcite{Chicco2017}.

\subsection{Validation and testing}
Validating a model means assessing its generalization performances.
This is usually done by reserving a part of the data and excluding it from the training process.
The model is trained on the so-called training set (the part of the data reserved for training) and then it is used to predict the data instances in the left-out fraction (called validation set).
The goodness of a model can then be evaluated from the error in predicting instances in the validation set.

Validation can be used to assess if a model is overfitting the training dataset.
In the case of overfitting, the validation error will tend to be sensibly higher than the training error.

A commonly used strategy is cross-validation, where the available data is split in a given number of sets of data points (called folds), and in turn one of these folds is left out for validation and the remaining ones are used for training.
The process is repeated training new models until all the folds have been used for validation.
The overall performances of the model can then be evaluated by aggregating all the validation predictions, or by calculating performance metrics for each fold.

If, as it is commonly done, the performances in the validation set are used for optimizing hyperparameters (in the broad sense, including selecting which features to use and which model to use), there is a potential for overfitting also the validation set.
For this reason, it is good practice to leave out of training and validation an additional portion of the data, which is never used in model development.
This test set will be used only to give an estimation of the true performances of the model with unseen data.
Also for validation and testing, \textcite{Chicco2017} has a nice description of the strategies recommended in computational biology.

\subsection{Hyperparameter optimization}
A hyperparameter of a machine learning model is a parameter whose value is not learned directly but is used to control the learning process.
On the contrary, the optimal value of a model parameter is learned during training.

To select the optimal values for the hyperparameters of a model, it is necessary to try out numerous combinations of them.
This will allow mapping a performance landscape in the hyperparameter space, from which the operator can in turn select a local maximum.
It is possible to systematically probe all the possible combinations of a set of values for the hyperparameters, a process called grid search, or to randomly select a small fraction of the possible combinations of a much larger set of values (a process called random search).

A grid search can be computationally expensive because it suffers from the curse of dimensionality when many hyperparameters have to be optimized, while a random search can be more easily used also for large numbers of hyperparameters.
In both cases, however, it is usually trivial to parallelize the hyperparameter search since testing each combination of values is an independent task.
Random search outperforms grid search when only a small number of hyperparameters affects disproportionately the final performance of the model (in other words, when the optimization problem has a low intrinsic dimensionality).

Other approaches to hyperparameter tuning are possible but were not used in this work.
They include Bayesian optimization, gradient-based optimization, and evolutionary optimization.

Once a satisfactory set of hyperparameter values has been found, it is possible to train a final predictor setting the hyperparameters to those values.
Excessive hyperparameter tuning can lead to overfitting of the validation data, and thus an independent test data is always an essential requirement for unbiased estimation of model performances.

For more details on hyperparameter tuning see \textcite{Bergstra2012, Claesen2015}.

\subsection{Linear regression}
Linear regression is a simple model for the prediction of one (univariate) or more (multivariate) dependent variables from a set of independent variables.
The prediction for the dependent variable is expressed as a linear combination of independent variables, with linear coefficients estimated from the data by minimising the sum of the squares of the errors.
In this contest, the errors are the residuals between the predictions of the model and the actual values for the dependent variable.

The implementation used in this work does not have any free hyperparameter, since no regularization was used.

A more in-depth discussion of linear regression is available in \textcite{Yan2009}.

\subsection{Gradient boosted trees}
Gradient boosted trees \parencite{Freund1997,Friedman2000,Friedman2001} are a machine learning approach that recently reached great popularity thanks to their excellent results in extracting information from structured data.
Compared to neural networks, which are the method of choice for unstructured data and when the spatial or sequential structure of the data is crucial for the prediction (for example, in computer vision and natural language processing), gradient boosted trees perform better with data in a tabular format, where a series of observations is associated with a series of features.

Gradient boosted trees are, like random forest, an ensemble method based on decision trees.
Decision trees are weak learners with high variance, amenable to be combined in an ensemble.
Differently from random forest, where the prediction of individual decision trees is averaged or in any case summarised horizontally, in gradient boosting each tree is trained in predicting the residuals from previous learners.

Gradient boosted trees can employ a variety of loss functions for regression, classification, and ranking \parencite[reviewed in][]{Natekin2013}.
The choice of the loss function is, in itself, a hyperparameter of the model.
For regression, common loss functions are the Mean Squared Error (MSE) and the Mean Absolute Error (MAE).
MSE penalizes more large errors compared to MAE\@.
When using regression trees in classification tasks (as some implementations do, instead of using directly classification trees), the output of the ensemble is subjected to a logistic or softmax function, and then a regression loss such as MSE or MAE is used.
Gradient boosted trees that natively perform classification can use the binary or categorical cross-entropy as a loss function.
Ranking loss functions include the pairwise loss (number of incorrectly ranked data pairs) and list-wise losses.
In Learning To Rank problems, the LambdaMART \parencite{Burges2010} gradient boosting formulation is a common choice.

The two main hyperparameters in gradient boosted trees are the learning rate and the number of iterations.
A small learning rate shrinks the contribution of successive trees as compared to the contribution of earlier ones.
A learning rate of \num{1} makes the contributions of all trees equal.
The number of iterations in the contest of gradient boosted trees is the number of successive trees trained to fit the residuals of the ensemble.
Learning rate and number of iterations are closely intertwined in that a small learning rate will require more iterations to reduce the error to a similar level, but the result will be more stable than when using a larger learning rate.
It can be said that a small learning rate and a large number of iterations can reduce overfitting at the cost of longer training times.

Regularization in gradient boosted trees can be enforced by applying a penalty term to the absolute values of the weights (L1 or lasso penalty) or their square (L2 or ridge penalty).
The weights in this scenario are the factors that multiply the output of individual weak learners in the error function.

Subsampling is another common regularization technique in gradient boosted trees which consists of considering only a random fraction of the training data for each learner.
Subsampling can be applied also column-wise (on the features), such that each weak learner uses only a random fraction of the features available.
Besides reducing overfitting, subsampling can also speed up training by reducing the number of data points considered at each iteration.

The maximum depth is a hyperparameter that governs the maximum depth to which individual trees are allowed to grow.
Increasing the maximum depth leads to more complex individual learners, and increases the risk of overfitting.
Enforcing a minimum child weight also acts as a regularizing hyperparameter.
The child weight is the number of instances in a leaf node in the case of equally-weighted data instances, and the sum of the instances weights otherwise.
When this hyperparameter is set, the algorithm will give up partitioning any further a node when its weight goes below the threshold.
Similarly to providing a minimum child weight, it is also possible to specify a minimum loss reduction required to continue splitting a node.

Additional hyperparameters besides the one described here are possible, depending on the loss function used and on the implementation.
