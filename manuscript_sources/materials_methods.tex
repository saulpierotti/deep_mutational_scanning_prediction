\thispagestyle{empty}
\cleardoublepage%
\chapter{Materials and Methods}

\section{Software}
The features used to train the predictive models presented in this work were produced using a variety of tools.
Multiple sequence alignments and Hidden Markov models were obtained with the HH suite \parencite[version 3.3.0]{Steinegger2019}, and with the HMMER package \parencite[version 3.3.2]{Eddy2011}.
The EVcouplings package~\parencite[version 0.1.1]{Hopf2018}, EVmutation~\parencite[installed as part of EVcouplings]{Hopf2017}, and plmc~\parencite[installed from \url{https://github.com/debbiemarkslab/plmc} in March 2021]{Hopf2017} were used for obtaining first- and second-order coupling terms among protein positions.
The neural network trRosetta~\parencite[installed from \url{https://github.com/gjoni/trRosetta} in March 2021 and using the trained model \texttt{2019\_07}]{Yang2020} was used for the prediction of protein contact maps.
The neural network NetSurfP-2.0 \parencite{Klausen2019} was used for the prediction of relative solvent accessibility, secondary structure, torsion angles, and disorder.

DSSP \parencite[Define Secondary Structure of Protein,][version 3.1.2]{Kabsch1983} was used for the assignment of protein secondary structure, torsion angles, and residue solvent accessibility from experimental protein structures.
DSSP assignments were not used in predictions but only for validating the output of NetSurfP-2 and for assessing how the fitness scores are affected by the solvent accessibility and the secondary structure of the mutated position.

The Python programming language~\parencite{vanRossum2010} was used throughout the project for implementing the models and for intermediate processing steps.
Different Python versions were used for different tasks according to the requirements of libraries and applications.
Virtual environments and containers were used for managing the different versions of Python and the libraries.
The last version of Python at the time of writing (3.9.2) was used whenever possible.
TensorFlow \parencite[version 2.4.1]{Abadi2015} and the Keras \parencite{Chollet2015} implementation \texttt{tensorflow.keras} were used for the development of neural network models.
At the time of writing TensorFlow did not support Python 3.9, so for running scripts requiring TensorFlow Python was downgraded to version 3.8.8.
Scikit-learn \parencite[version 0.24.1]{Pedregosa2011} was used for data preprocessing, for cross-validation and testing, and for deploying some of the models.
XGBoost \parencite[version 1.4.0rc1]{Chen2016} was used to implement regressors based on gradient boosted trees.
NetworkX \parencite[version 2.5.1]{Hagberg2008} was used for representing the residue contacts in proteins as graphs, and for calculating properties of those graphs.
The libraries Pandas \parencite[version 1.2.3]{pandas2021}, NumPy \parencite[version 1.20.1]{Harris2020}, and Scipy \parencite[version 1.6.2]{Virtanen2020} were used throughout the project.
Some of the code was parallelized for running on supercomputer clusters using MPI for Python \parencite[also called mpi4py,][version 3.0.3]{Dalcin2011}.
Joblib \parencite[version 1.0.0]{JDT2020} was used for implementing shared-memory parallelization and for persisting and compressing Python objects.
The interactive Python development environment IPython \parencite[version 7.19.0]{Perez2007}, run through JupyterLab \parencite[version 3.0.12]{Kluyver2016} was used for prototyping and visualizations.
Plots in the prototyping phase were produced using Matplotlib \parencite[version 3.4.1]{Hunter2007} and Seaborn \parencite[version 0.11.1]{Waskom2021}.

All the visualizations shown in this manuscript were produced with the R programming language \parencite[version 4.0.5]{RCT2021} using chiefly the package ggplot2 \parencite[version 3.3.3]{Wickham2016} and the tidyverse ecosystem \parencite[version 1.3.1]{Wickham2019}.
The packages cowplot \parencite[version 1.1.1]{Wilke2020} and scales \parencite[version 1.1.1]{Wickham2020} were used for tweaking the appearance of the visualizations.
Biostrings \parencite[version 2.56.0]{Pages2020} was used to obtain the values for the BLOSUM100 scoring matrix \parencite{Henikoff1992} shown in \autoref{subfig:blosum100}.
The machine learning package caret \parencite[version 6.0--88]{Kuhn2021} was used for obtaining the classification evaluation metrics shown in \autoref{tab:netsurf_validation}.
Directional \parencite[version 4.9]{Tsagris2021} and circular \parencite[version 0.4--93]{Agostinelli2017} were used for obtaining respectively correlation statistics among circular quantities and between circular and linear quantities.
The only circular quantities used in this work are the backbone torsion angles.
Reshape2 \parencite[version 1.4.4]{Wickham2007} was used for data reshaping.
The package rlang \parencite[version 0.4.11]{Henry2021} was used for functionalising some of the R code.
The RStudio development environment was also used heavily \parencite[version 1.4.1103 ``Wax Begonia'']{RStudioTeam2021}.

\section{Databases}
Protein sequence databases were used extensively in this project.
UniProt \parencite[release \texttt{2021\_02}]{Bateman2020} was used as an authoritative source for the wild-type sequence of the mutagenised proteins.
Uniclust30 \parencite[release \texttt{2020\_06}]{Mirdita2016} was used for the construction of multiple sequence alignments with the hhblits tool (HH suite).
UniRef100 \parencite[release \texttt{2021\_4}]{Suzek2014} was used by jackhmmer (HMMER package) in the first step of the EVcouplings pipeline.
Experimental protein structures stored in the Protein Data Bank \parencite[PDB,][accessed in March 2021]{Burley2018} were used for assessing the quality of predicted structural features.
InterPro \parencite[release 84.0]{Blum2020} was used for determining protein domain boundaries when trimming the query sequences.
Structure Integration with Function, Taxonomy and Sequence \parencite[SIFTS,][accessed in March 2021]{Dana2018} and the Protein Data Bank in Europe-Knowledge Base \parencite[PDBe-KB,][accessed in March 2021]{Varadi2019} were used for mapping PDB structures to UniProt positions.

\section{Computational infrastructure}
Light computational work and simple manipulations were performed locally on a Linux machine.
Due to the storage requirements for large protein sequence databases, the multiple sequence alignments were performed on a remote Linux server at SciLifeLab (Stockholm, Sweden), kindly provided by Prof.~Arne Elofsson.
GPU-heavy work such as neural network training was also performed on the same server, which was equipped with two Nvidia GeForce RTX 2080 GPUs.
The most computationally demanding steps (random search of hyperparameter space for the models) were performed on the supercomputer cluster Kebnekaise, of the High-Performance Computing Center North (HPC2N) located in Ume√•, Sweden.
Access to the infrastructure was provided under the project SNIC2020--5--300 of Prof.~Elofsson.

\section{Dataset}\label{sec:mm_dataset}
The deep mutational scanning datasets used in this study were obtained from the training set of the variant effect predictor Envision~\parencite{Gray2018}.
For convenience, fitness measurements for each mutation were extracted from the supplementary files available in~\cite{Gray2018} and not from the respective dataset publications.
Envision used a normalized version of the raw fitness scores in its training, but for this work, I opted for using the raw scores of each mutation.

To maximise the comparability of the results of this study with those of Envision, the deep mutational scanning datasets that were excluded from the training of Envision were filtered out also in this work.
The retained data consisted of nine independent experiments on eight different proteins.
They employed vastly different methodologies for the evaluation of variant fitness and included proteins of bacterial, yeast, rat, and human origins.
Each dataset consisted of a set of single amino acid variants associated with a fitness score.
A score of \num{0} was assigned to the wild type, with positive values indicating variants more abundant than the wild type after selection, and negative values variants less abundant than the wild type.
A summary of the datasets is reported in \autoref{tab:datasets}.
The fitness scores were obtained from the sequencing counts for each mutation according to \autoref{eq:fitness}, where $m_a$ and $m_b$ are the read counts for the mutant sequence after and before selection, and  $wt_a$ and $wt_b$ are the read counts for the wild-type sequence, after and before selection.

\begin{equation}\label{eq:fitness}
	Fitness = \log{\frac{m_a/m_b}{wt_a/wt_b}}
\end{equation}

\subsection{Correction for dataset \texttt{kka2\_1:2}}
It was observed in preliminary analyses that the dataset \texttt{kka2\_1:2} (\autoref{tab:datasets}) contained duplicated mutations with different fitness scores in the aggregated version of the dataset from \textcite{Gray2018}.
Curiously, these duplicated entries involved only and all of the mutations towards isoleucine, tyrosine, serine, asparagine, and glutamate.
It was not possible to determine the cause of the duplicated entries, which are absent from the original dataset from \textcite{Melnikov2014}.
In a private communication, the corresponding author of \textcite{Gray2018} informed me that the inclusion of these duplicated entries is most probably accidental.
For this reason and to avoid possible biases in training towards these specific mutations, the entries relative to this dataset were sourced directly from the original publication, \textcite{Melnikov2014}.

\textcite{Melnikov2014} reports different categories of fitness scores, measured under different selection conditions.
The scores that were included in the training set for this work are the ones obtained with kanamycin selection at half of the minimum inhibitory concentration.
The pre-computed fitness scores were used, and not the raw sequencing counts.
All the statistics for \texttt{kka2\_1:2} used in this study reflect the data sourced from \textcite{Melnikov2014} and not the data included in \textcite{Gray2018}.

\subsection{Other minor inconsistencies}\label{sec:minor_inconsistencies}
It was noted that for dataset \texttt{hsp90} the mutations from position \numrange{212}{222} are missing, while they are present in the original publication \parencite{Mishra2016}.
It was also noted that for dataset \texttt{E1\_Ubiquitin} the mutations from position \numrange{40}{48} are missing.
In the original publication, it is mentioned that these exact positions were used for method development but are nonetheless included in the fitness measurements.
It was not possible to determine the source of these two discrepancies, but given the small number of mutations involved no action was taken.
This was also done to maintain the dataset as similar as possible to the one used for training Envision, to allow for more meaningful comparisons.

\subsection{Filtering}
The datasets were purged of the mutations to or from non-standard or unknown residues.
The declared original residue for each mutation was confronted with the residue in that position on the UniProt sequence.
Mismatching entries were excluded from further processing.
See \autoref{tab:excluded} for a summary of the number of retained data points for each dataset.

\begin{table}[p]
	\small%
	\ttabbox[\textwidth]{%
		{\hypersetup{urlcolor=., citecolor=.}%
				\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llll}%
					\midrule
					Dataset name   & Source organism  & UniProt ID & Reference \\
					\midrule
					\texttt{beta-lactamase} & \textit{Escherichia coli}           & \href{https://www.uniprot.org/uniprot/P62593}{\texttt{P62593}} &\cite{Firnberg2014}   \\
					\texttt{WW\_domain}     & \textit{Homo sapiens}               & \href{https://www.uniprot.org/uniprot/P46937}{\texttt{P46937}} &\cite{Fowler2010}     \\
					\texttt{PSD95pdz3}      & \textit{Rattus norvegicus}          & \href{https://www.uniprot.org/uniprot/P31016}{\texttt{P31016}} &\cite{McLaughlin2012} \\
					\texttt{kka2\_1:2}      & \textit{Klebsiella pneumoniae}      & \href{https://www.uniprot.org/uniprot/P00552}{\texttt{P00552}} &\cite{Melnikov2014}   \\
					\texttt{hsp90}          & \textit{Saccharomyces cerevisiae}   & \href{https://www.uniprot.org/uniprot/P02829}{\texttt{P02829}} &\cite{Mishra2016}     \\
					\texttt{Ubiquitin}      & \textit{Saccharomyces cerevisiae}   & \href{https://www.uniprot.org/uniprot/P0CG63}{\texttt{P0CG63}} &\cite{Roscoe2013}     \\
					\texttt{Pab1}           & \textit{Saccharomyces cerevisiae}   & \href{https://www.uniprot.org/uniprot/P04147}{\texttt{P04147}} &\cite{Melamed2013}    \\
					\texttt{E1\_Ubiquitin}  & \textit{Saccharomyces cerevisiae}   & \href{https://www.uniprot.org/uniprot/P0CG63}{\texttt{P0CG63}} &\cite{Roscoe2014}     \\
					\texttt{gb1}            & \textit{Streptococcus} sp.\ group G & \href{https://www.uniprot.org/uniprot/P06654}{\texttt{P06654}} &\cite{Olson2014}      \\
					\bottomrule
				\end{tabular*}}%
	}%
	{%
		\caption[Summary of the deep mutational scanning datasets used in this work]{\textbf{Summary of the deep mutational scanning datasets used in this work.} The column ``Dataset name'' reports the name used throughout this work to refer to each dataset and is identical to the names used in~\cite{Gray2018}.
		}\label{tab:datasets}%
	}%
	\tabsep%
	\ttabbox[\textwidth]{%
		\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lll}%
			\toprule
			Dataset name   & Total number of mutations & Number of retained mutations \\
			\midrule
			\texttt{beta-lactamase} & \num{5436}                & \num{5397}                   \\
			\texttt{WW\_domain}     & \num{377}                 & \num{373}                    \\
			\texttt{PSD95pdz3}      & \num{1577}                & \num{1577}                   \\
			\texttt{kka2\_1:2}      & \num{5280}                & \num{5280}                   \\
			\texttt{hsp90}          & \num{4417}                & \num{4231}                   \\
			\texttt{Ubiquitin}      & \num{1403}                & \num{1267}                   \\
			\texttt{Pab1}           & \num{1276}                & \num{1220}                   \\
			\texttt{E1\_Ubiquitin}  & \num{1198}                & \num{1142}                   \\
			\texttt{gb1}            & \num{1045}                & \num{1026}                   \\
			\bottomrule
		\end{tabular*}%
	}%
	{\caption[Number of mutations in each dataset before and after filtering]{%
			\textbf{Number of mutations in each dataset before and after filtering.} Table that summarises the number of mutation originally present in each deep mutational scanning dataset, and the number of mutations that were retained after filtering.
		}\label{tab:excluded}%
	}%
	\tabsep%
	\ttabbox[\textwidth]{%
		\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llll}%
			\toprule
			Dataset name            & Mutagenized range   & MSA query trimming  & Trimming strategy                       \\
			\midrule
			\texttt{beta-lactamase} & \numrange{1}{286}   & full protein        & not trimmed                             \\
			\texttt{WW\_domain}     & \numrange{170}{203} & \numrange{167}{207} & InterPro superfamily \texttt{IPR036020} \\
			\texttt{PSD95pdz3}      & \numrange{311}{393} & \numrange{303}{426} & InterPro superfamily \texttt{IPR036034} \\
			\texttt{kka2\_1:2}      & \numrange{1}{264}   & full protein        & not trimmed                             \\
			\texttt{hsp90}          & \numrange{2}{231}   & \numrange{2}{231}   & InterPro superfamily \texttt{IPR036890} \\
			\texttt{Ubiquitin}      & \numrange{2}{76}    & \numrange{1}{76}    & InterPro domain \texttt{IPR000626}      \\
			\texttt{Pab1}           & \numrange{126}{200} & \numrange{126}{203} & InterPro domain \texttt{IPR000504}      \\
			\texttt{E1\_Ubiquitin}  & \numrange{2}{70}    & \numrange{1}{76}    & InterPro domain \texttt{IPR000626}      \\
			\texttt{gb1}            & \numrange{229}{282} & \numrange{226}{283} & InterPro domain \texttt{IPR000724}      \\
			\bottomrule
		\end{tabular*}%
	}%
	{\caption[Trimming strategy for the multiple sequence alignments queries]{%
			\textbf{Trimming strategy for the multiple sequence alignments (MSA) queries.} Table that details the range of positions in each mutagenized protein for which mutations were present in the respective dataset, how the query sequences were trimmed before being used in database searches for building multiple sequence alignments, and the rationale behind the choice of those positions.
			All the positions are indicated according to the UniProt numbering of the respective sequences.
		}\label{tab:mut_range}%
	}%
\end{table}

\subsection{Normalization of fitness scores}\label{sec:mm_quantile_norm}
For the single protein models (\autoref{sec:mm_single_protein_models}), the fitness scores were used in their original, non-normalized format.
For the general models (\autoref{sec:mm_general_models}), the fitness scores were used in their original format for models that could be easily adapted in using a ranking loss function, while they were normalized for models that relied on a regression loss function as the minimum squared error.

The normalization strategy used aimed at making the scores more easily comparable across datasets, and at reducing the excessive influence that datasets with a large score range would otherwise have on the loss function.
The scores were first grouped by dataset and quantile normalized to the range \numrange{0}{1} using \num{100} quantiles.
Subsequently, for each dataset from the normalized scores, the quantile value corresponding to a score of \num{0} was subtracted.
This had the effect of centring the data in each dataset such that positive scores would correspond to an increase in stability compared to the wild-type, and vice-versa for negative scores.

\FloatBarrier%
\section{Features}
All the predictors were trained using the same set of features.
These included only information that could be derived from the sequence of the proteins and comparison with homologous sequences, avoiding the need for structural data.
The approach proposed in this work leverages the use of unsupervised models, supervised predictors for structural data, sequence information, and position-specific scoring matrices.

\subsection{Mutation identity}
For each data point, the original and mutated amino acid were provided to the models in one of two different ways.
One-hot encoding was used for gradient boosted trees, linear regressors and support-vector machines.
Each residue was represented by a vector with 20 elements, one for each possible standard residue.
Given their rarity, mutations to or from non-standard or unknown residues were excluded from the dataset and thus the encoding of only standard residues was sufficient for the scope of this work.
For neural networks, an ordinal encoding was used as input to an embedding layer.
The dimensionality of the residue embedding was treated as a hyperparameter.

\subsection{Multiple sequence alignments}\label{sec:msa}
Multiple sequence alignments are central to this predictor since they constitute the input for the computation of many of the features used.
In general, alignment methods based on Hidden Markov Models were adopted because of their superior speed in large database searches and their increased sensitivity in the identification of remote homologies as compared to more traditional methods such as PSI-BLAST \parencite[see the introduction of][and references therein]{Steinegger2019}.
The tool hhblits from the HH suite was used for most of the alignments.
An exception was the alignments used for the inference of evolutionary couplings, for which the pipeline included in the EVcouplings package was used (see \autoref{sec:ev_mutation}).
The hhblits tool was used with default parameters (which correspond to two iterations) and enforcing a minimum query coverage of 70\% against the Uniclust30 database.

The query sequences used correspond to the respective UniProt canonical sequences for each of the mutagenised proteins.
Most of the deep mutational scanning datasets used in this work focused on only one domain of a protein.
For this reason, both the full-length UniProt sequence and the sequence trimmed to the mutagenised domain were used, obtaining two distinct multiple sequence alignments for each protein (excluding experiments where the full protein was mutagenised, where a single multiple sequence alignment was obtained).

In most cases the multiple sequence alignments used in input for the extraction of features were the ones corresponding to the full-lenght query sequence, with three exceptions:
\begin{enumerate}[label= (\roman*)]
	\item The datasets \texttt{Ubiquitin} and \texttt{E1\_Ubiquitin}.
	      Ubiquitin is encoded in the yeast genome in four different genes, and some of those are polyproteins with multiple domain repeats.
	      The first ubiquitin repeat of the yeast gene UBI4 was used as query sequence for this protein in all instances.\
	\item The dataset hsp90. It covers the ATPase domain of the yeast molecular chaperone HSP82.
	      The full-length alignment of this protein tended to recruit many sequences unrelated to the ATPase domain but homologous to the rest of the HSP82 sequence.
	      For this reason, the trimmed version of the query was used.\
	\item Preliminary visual comparison of the contact maps obtained with trRosetta with experimental contact maps retrieved from structures from the Protein Data Bank showed that more accurate predictions could be obtained by using multiple sequence alignments trimmed to single domains instead of full-length alignments.
	      Thus, trRosetta was always given in input the multiple sequence alignments trimmed to the mutagenised domains.
\end{enumerate}

The protein positions considered in each case are reported in \autoref{tab:mut_range}.
The trimming step was always done at the level of the query sequence before the generation of the multiple sequence alignments.
In all cases, the alignments were further processed by trimming out all the columns which corresponded to gaps in the query sequence.

\subsection{Position-specific scoring matrices}
The position-specific scoring matrices (PSSMs) were obtained from the emission probabilities of the Hidden Markov models trained from the hhblits multiple sequence alignments produced as described in \autoref{sec:msa}.
The Hidden Markov models were trained with the tool ``\texttt{hmmbuild}'' from the HMMER suite.
This tool was preferred to the corresponding tool ``\texttt{hhmake}'' from the HH suite (which was used for obtaining the multiple sequence alignments in the first place) since the latter does not include pseudo counts in the model file and thus needed further processing for producing usable PSSMs.

From the model file, the PSSMs were extracted by taking the emission probabilities of the match states for positions of the query corresponding to match states, and by taking the emission probabilities of the insert states for positions of the query corresponding to insert states.
A Python script produced in house was used for the purpose.

The entire PSSM vector for each position was used as a feature, together with the PSSM scores for the wild-type residue and the mutated residue, and the difference between the two.

\subsection{Evolutionary couplings}\label{sec:ev_mutation}
EVmutation is an unsupervised model for the prediction of mutation effect.
It is available as part of the EVcouplings package, which implements a full pipeline from sequence to the prediction of mutation effect (among other functions).

Internally, first EVcouplings produces a multiple sequence alignment of the query using an available installation of the tool ``\texttt{jackhmmer}'' in the system (part of the HMMER suite).
After obtaining the multiple sequence alignment, it calculates the coupling between positions using plmc, a method for the inference of undirected graphical models that describe first-order couplings.
Finally, EVcouplings calls EVmutation and predicts second-order coupling terms between multiple sequence alignment columns.
The output of this stage is a table containing every possible single mutation in the query and the associated plmc and EVmutation scores.
Both scores were used as features in the predictor.

For all stages, the default parameters from the sample EVcouplings configuration file were used.
For the jackhmmer search, the parameters used included five iterations, a relative sequence and domain bit score threshold of 0.5 bits per residue, and UniRef100 as a database.

\subsection{Predicted solvent accessibility, secondary structure, torsion angles, and disorder status}
The neural network NetSurfP-2.0 is a state-of-the-art predictor for protein secondary structure, residue solvent accessibility, torsion angles, and disordered residues.
The hhblits-based variant of the network was used, which uses hhblits-derived multiple sequence alignments.
The multiple sequence alignments given in input were obtained with hhblits as described in \autoref{sec:msa}.
All the output values of the network were used as features.
These include the relative and absolute solvent accessibilities, the three- and eight-class secondary structure class probabilities, torsion angles, and disorder probability.

\subsection{Connectivity graphs}
Protein contacts were predicted with trRosetta.
The contacts were then used for defining connectivity graphs of the proteins, from which various features were extracted.

The inputs given to trRosetta were the multiple sequence alignments obtained with hhblits from the sequence of the mutagenised proteins.
As explained in \autoref{sec:msa}, the query sequences were trimmed to the respective mutated domains before running hhblits.\
trRosetta produces in output predicted interresidue angles and predicted interresidue distances for each pair of positions in the input sequence.
The predicted interresidue angles were not used in this work.
The predicted distances are represented by trRosetta binned in 36 bins, each collecting distances from \SIrange{2}{20}{\angstrom} in bins of \SI{0.5}{\angstrom}, with an additional bin for no contact.
The network is trained to predict the distances among the respective C$\beta$ for each pair of residues (C$\alpha$ for glycine, which does not have a C$\beta$ atom).
The last layer of the network has a softmax activation, and thus the numerical value stored in each output neuron can be interpreted as the probability that a pair of residues are at a distance included in the respective bin.

To obtain a contact map from the trRosetta output, first, the binned distances had to be converted to a single number representing the most probable distance.
Firstly, the pairs of residues for which the probability of not being a contact was bigger than the summed probabilities of the remaining \num{36} distance bins were assigned an infinite distance.
Secondly, for those pairs where the above condition was not verified, the most probable distance was obtained as the sum of the bin distances, weighted by the probability of each bin.
The bin probabilities were rescaled to the total probability of the \num{36} distance bins before this operation (the sum of the distance bins probabilities, excluding the non-contact bin).
The distance considered for each bin was the central one: for instance, for the bin that spanned \SIrange{2}{2.5}{\angstrom}, a distance of \SI{2.25}{\angstrom} was considered.
Finally, the residues at less than \SI{8}{\angstrom} predicted distance apart were considered contacts.

The contact maps were converted to undirected graphs using the Python library NetworkX.
For each residue (node in the connectivity graph), the following metrics were calculated with NetworkX and used as features for the models produced in this work: closeness centrality, betweenness centrality, degree centrality, load centrality, harmonic centrality, clustering coefficient.
Closeness centrality was obtained as described in \textcite{Freeman1978}.
Betweenness centrality was obtained as described in \textcite{Brandes2001}.
Degree centrality was obtained by counting the number of edges connected to a node and normalizing this value by $n-1$, where $n$ is the total number of nodes in the graph.
Load centrality was obtained as described in \textcite{Newman2001}.
Harmonic centrality was obtained as described in \textcite{Boldi2014}.
The clustering coefficient implementation used is described in \textcite{Onnela2005}.
Further implementation details can be obtained from the NetworkX documentation at \url{https://networkx.org/documentation}.

\subsection{Imputation of missing features}\label{sec:mm_missing}
The features obtained from EVcouplings were missing for some mutations.
This is due to the handling of regions with a poor quality multiple sequence alignment done by the EVcouplings package.
For predictors that can deal with missing features (gradient boosted trees), no imputation was done.
For predictors that could not deal with missing features (linear regression, support vector machines, neural networks), missing features were imputed using the median feature value of the respective training sets.

\section{Experimental validation of predicted structural features}\label{sec:mm_exp_val}
To validate the predicted structural features used in model training, when possible they were compared to the corresponding experimental properties.
It should be noted that features derived from experimental protein structures, however, were never used in model training.
The features that were validated in this way consisted of the NetSurfP-2 predictions and the trRosetta contact maps.

\begin{table}[h]
	\small%
	\ttabbox[\textwidth]{%
		\hypersetup{urlcolor=.}%
		\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lll}%
			\toprule
			Dataset name   & PDB ID & Author chain ID \\
			\midrule
			\texttt{beta-lactamase} & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/1btl}{\texttt{1BTL}} & \texttt{A} \\
			\texttt{WW\_domain}     & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/4rex}{\texttt{4REX}} & \texttt{A} \\
			\texttt{PSD95pdz3}      & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/1be9}{\texttt{1BE9}} & \texttt{A} \\
			\texttt{kka2\_1:2}      & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/1nd4}{\texttt{1ND4}} & \texttt{A} \\
			\texttt{hsp90}          & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/1ah6}{\texttt{1AH6}} & \texttt{A} \\
			\texttt{Ubiquitin}      & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/3olm}{\texttt{3OLM}} & \texttt{D} \\
			\texttt{Pab1}           & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/6r5k}{\texttt{6R5K}} & \texttt{D} \\
			\texttt{E1\_Ubiquitin}  & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/3olm}{\texttt{3OLM}} & \texttt{D} \\
			\texttt{gb1}            & \href{https://www.ebi.ac.uk/pdbe/entry/pdb/2igd}{\texttt{2IGD}} & \texttt{A} \\
			\bottomrule
		\end{tabular*}%
	}%
	{\caption[PDB structures and chain identifiers used for the validation of predicted features]{%
			\textbf{PDB structures and chain identifiers used for the validation of predicted features.} Table that shows the PDB and chain identifiers used for the validation of the NetSurfP-2 and trRosetta predicted structural features, which were in turn used to train the models presented in this work.
			Note that the chain identifiers refer to the author chain and not the PDB chain.
		}\label{tab:pdb_struct}%
	}%
\end{table}

Firstly, experimental protein structures were sourced from the PDB.\@
The PDB structure and chain identifiers used for each dataset are reported in \autoref{tab:pdb_struct}.
For the datasets \texttt{Ubiquitin} and \texttt{E1\_Ubiquitin}, the mutagenised domain correspond to the first ubiquitin domain of the yeast UBI4 gene (UniProt ID:~\texttt{P0CG63}).
A structure is not available for this portion of the protein, but it is available for the third ubiquitin domain of the same protein.
For this reason, the residues in the PDB structure were first mapped following the SIFTS mappings and then shifted backwards of \num{304} residues, to overlap with the mutagenised domain.
In the dataset gb1, the first Immunoglobulin G (IgG) binding domain of Protein G from \textit{Streptococcus sp.} (UniProt ID:~\texttt{P06654}) was mutagenised.
However, a structure is available only for the second IgG-binding domain of the same protein.
For this reason, the residues in the PDB structure were first mapped following the SIFTS mappings and then shifted backwards of \num{70} residues, to overlap with the mutagenised domain.

The software DSSP was used to assign secondary structures, torsion angles, and solvent accessibility to the residues in the PDB structures.
These properties were mapped to the UniProt sequences using SIFTS mappings and the adjustments described in the previous paragraph.

Since the secondary structure assigned by DSSP is categorised in eight classes, for the evaluation of the three-class NetSurfP-2 secondary structure predictions these had to be reduced to a three-class system.
The following mapping was used: $3_{10}$-helix, $\alpha$-helix, and $\pi$-helix were mapped to helix; $\beta$-strands and $\beta$-bridges were mapped to strands; the remaining classes were mapped to coil.
For the evaluation of the eight-classes secondary structure predictions of NetSurfP-2 instead, the DSSP assignments were used in their original format.

Since DSSP reports only the Accessible Surface Area (ASA) and not the Relative Solvent Accessibility (RSA) for each residue, the relative solvent accessibility had to be calculated to evaluate the quality of the NetSurfP-2 RSA predictions.
The DSSP RSA was obtained by dividing the DSSP ASA by the maximum solvation values for each residue type as reported in \textcite{Ahmad2003}.
These values correspond to the ASA of the residue type in an extended tripeptide (Ala-X-Ala) configuration.
The NetSurfP-2 predicted ASA was instead compared directly to the DSSP ASA.\@
Note that NetSurfP-2 predicts directly only the RSA, and the ASA is instead calculated from it using the maximum solvation values reported above.

For the evaluation of the trRosetta distograms, distance maps were obtained from the PDB structures by evaluating all the possible pairwise residue distances.
The PDB distance maps were then mapped to the respective UniProt sequences following the SIFTS mappings and additionally the adjustments described in this section for the datasets \texttt{Ubiquitin}, \texttt{E1\_Ubiquitin}, and \texttt{gb1}.
The trRosetta performances were evaluated as the fraction of experimental contacts that are present in the top $L/n$ medium and long-range predicted trRosetta contacts, where $n$ is \numlist{5;2;1} and $L$ is the length of the primary sequence from the PDB\@.
Medium range contacts were defined as contacts between residues at \num{12} or more positions apart in the primary sequence.
Long-range contacts were defined as contacts between residues at \num{24} or more positions apart in the primary sequence.
An experimental contact was defined as a pair of residues at a distance of \SI{8}{\angstrom} or less.
Experimental distances were measured between $C_\beta$ atoms for all residues except glycine, where the $C_\alpha$ atom was taken as reference.

\section{Validation and testing strategy}\label{sec:mm_validation_and_testing}
The general framework followed throughout the project is that of using cross-validation for model optimization on a part of the dataset, and then evaluating the generalization performances on a different test set.
In general, the test set was separated from the training dataset before performing cross-validation.

\subsection{Single protein models}\label{sec:mm_single_protein_models}
Single protein models are models trained on a single deep mutational scanning experiment to predict a holdout proportion of the mutations in the same dataset.
Independent models were produced for each of the nine deep mutational scanning datasets used in this work.

Each dataset was first shuffled and then split in half.
One half was split again into five cross-validation folds.
Once a satisfactory combination of hyperparameters was found, the model was trained again on the full half used for validation.
The model was then evaluated on the portion left out for testing.

Two different validation and testing strategies were used, both conforming to the description given above.
In the first case, which I refer to as ``naive validation and testing'', the mutations in a given dataset were just distributed randomly among training and testing sets, and among the different validation folds.
In the second case, care was taken to avoid different mutations of the same residue (same UniProt position) ending up both in the training and testing sets or in different cross-validation folds.

\subsection{General models}\label{sec:mm_general_models}
The general models trained on the aggregated dataset were cross-validated in a Leave-One-Protein-Out (LOPO) fashion, similarly to what was done in \textcite{Gray2018}.

Differently than in that work, however, in my implementation half of the mutations in the left-out protein at each iteration were used for hyperparameter tuning, while the other half was set aside for testing.

The LOPO cross-validation approach consists of training the model iteratively on all the proteins except one and then using the remaining protein for validation and testing.
It should be noted that the left-out portion of the aggregated dataset was selected according to the UniProt identifier, and not according to the deep mutational scanning experiment.
This implies that when more than one experiment targeted the same protein, such as in the case of ubiquitin, all of those experiments ended up in the same fold.

The separation of validation and testing data on the holdout proteins was done in such a way to avoid that different mutations affecting the same protein positions ended up in the same set.

Hyperparameter tuning and test performance evaluation were conducted in an aggregated manner for all the proteins, not in a protein-specific way.

\section{Model architectures and optimization}\label{sec:mm_optimization}
Different models were tested for the general configuration described in \autoref{sec:mm_general_models}, while for the single protein models only gradient boosted trees were used.
This section describes the optimization strategy used for each model type, and other custom configurations used.

The optimization of hyperparameters was done by cross-validation and random search.
The number of sampled configurations varied for different models.
Many rounds of random search were usually performed testing different hyperparameters or narrowing down the range for specific hyperparameters until a satisfactory combination was found.

\subsection{Linear regression}
A simple linear regressor without any free hyperparameter and regularization was deployed using the implementation available in the Scikit-learn Python library (the class \texttt{sklearn.linear\_model.LinearRegression} was used).
Linear regression was used only for the general model.
To make the results from different datasets more comparable, the fitness scores were quantile-normalized before being fed to the predictor as explained in \autoref{sec:mm_quantile_norm}.

\subsection{Gradient boosted trees}
Gradient boosted trees were implemented with the XGBoost Python library.
For more details on the implementation and available parameters, the reader can refer to the library documentation at \url{https://xgboost.readthedocs.io}.

To speed up training, the histogram version of the implementation was used (\texttt{tree\_method} set to \texttt{hist}).
For single protein models (\autoref{sec:mm_single_protein_models}), the loss function used was the minimum squared error (\texttt{objective} set to \texttt{reg:squarederror}).
For the general model, a pairwise ranking loss was used (\texttt{objective} set to \texttt{rank:pairwise}).

The hyperparameters that were optimized in gradient boosted trees where the following (in parenthesis the name of the parameter in the XGBoost implementation): maximum depth (\texttt{max\_depth}), minimum child weight (\texttt{min\_child\_weight}), fraction of subsampled datapoints (\texttt{subsample}), fraction of subsampled columns (\texttt{colsample\_bytree}), learning rate (\texttt{eta}), minimum loss reduction required to make a further partition on a leaf node of the tree (\texttt{gamma}), L2 regularization term on weights (\texttt{lambda}), L1 regularization term on weights (\texttt{alpha}), number of iterations (\texttt{num\_rounds}).

It was observed in preliminary trials that the larger the number of iterations the better the predictions (with diminishing returns for very large values).
Because of this, the number of iterations was fixed to the largest value that allowed to train a model in a reasonable amount of time.
This value was determined to be \num{1e3} iterations for the random search, and it was increased to \num{1e4} iterations for testing (when estimating both the final validation and testing performances).

The random search was started with a very broad range for each hyperparameter and then repeated once the most promising values were determined.
This was iterated several times until no further improvement could be reached.
For some of the models, the regularization terms were optimized separately from the other hyperparameters, since for many combinations of the regularization terms the model failed to yield a measurable performance (constant predictions).
This led to discarding most of the results, and increasing the search space such that a satisfactory number of observations could be obtained was not practical with the available resources.
In those cases first, the regularization terms were left to default values, and the remaining hyperparameters were optimized.
Once optimal values were found, the regularization terms were optimized while the other hyperparameters were set to the optimal values.
Finally, the learning rate and the number of iterations were examined again by plotting learning curves.
If necessary, the learning rate was adjusted.


For each random search iteration, around \num{2000} configurations were sampled.
The actual number varied according to the number of available nodes in the supercomputer cluster used for the computation.
The random search was parallelized with MPI for Python, such that different combination of hyperparameters could be tested at the same time.
This allowed me to take advantage of the large number of cores available in the supercomputer cluster, and the embarrassingly parallel characteristic of the random search problem.
The number of sampled configurations was adjusted to maximise resource usage and avoid that some nodes waited for others to complete their computations (it was adjusted to be an integer multiple of the number of cores available).

\section{Performance evaluation}
Performance was evaluated in terms of correlation between the predicted and ground truth values.
For single protein models, the Pearson correlation coefficient was used.

In cross-validation, the performance was not calculated for each fold but on the aggregated predictions.
That is, the model was trained with all but one fold and used to predict the remaining one.
The predicted fitness values were stored, and the model was trained again leaving out a different fold.
The predictions on this fold were concatenated to the ones obtained before.
This was repeated until the data points on all the folds had a prediction associated with them.
Then, performance metrics were evaluated on these aggregated predictions.
This evaluation strategy was preferred to a fold-specific metric since having a larger number of predictions allowed for greater confidence in the observed correlations.

When testing, the training part of the dataset was first cross-validated with the given combination of hyperparameters and the aggregated predictions for all the folds were evaluated.
The model was trained again on the full training set and used to predict the test set.
The performance of the test predictions was evaluated, and the validation and test scores were reported.
The comparison of validation and test scores allowed me to estimate if the model was overfitting the validation set.

\section{Comparison with other variant effect predictors}
A set of state-of-the-art quantitative variant effect predictors were selected: the supervised gradient boosting tree model Envision \parencite{Gray2018} and the unsupervised EVcouplings epistatic and independent models \parencite{Hopf2018}.

A notable quantitative model that was not included in this comparison is the unsupervised variational autoencoder DeepSequence \parencite{Riesselman2018}.
It was evaluated the possibility to include it in this comparison and also to feed its predictions as a feature for my models, but the extensive computational resources required for training the protein-specific DeepSequence models prevented me from doing so.

For Envision, since the model was trained on the same training set adopted in this work, using directly the predictions from the Envision webserver would have been an unfair comparison.
For this reason, the cross-validation performances of Envision reported in \textcite{Gray2018} are shown.
Since the original work does not report the Kendall correlation coefficients, only the Pearson and Spearman correlations are reported.
The performances obtained by Envision with rescaled fitness scores were chosen.

Independent and epistatic EVcouplings predictions were already used as features, and the procedure followed for their acquisition is detailed in \autoref{sec:ev_mutation}.
The respective predictions were directly compared to the experimental, unscaled fitness scores.

For my predictors, the performances are the ones measured on the respective testing sets.

\section{Evaluation of feature importance}\label{sec:mm_feature_importance}
To assess which features contributed more to the performance of the models developed in this work, their permutation importance was evaluated.

Given that some of the features used are collinear, they were not evaluated independently but by grouping them into nine groups.
This was needed since, if one feature is removed but another highly correlated feature is still present, the model would still be able to use the remaining feature to obtain the same information.
The same would be true in reverse when the second feature is removed and the first one left in.
This would lead to the faulty conclusion that both features are not important, while actually, they are important but only when the correlated feature is not present.

The identity of the wild-type and mutant amino acid were evaluated independently and not grouped.
Also, the NetSurfP-2 predicted disorder was evaluated independently.
The $Q_3$ and $Q_8$ secondary structure predictions obtained with NetSurfP-2 were grouped.
All of the trRosetta-derived centrality metrics were grouped.
All the PSSM scores for the \num{20} standard residue types, the PSSM scores for the wild-type and mutant residues, and their difference were grouped.
All the EVcouplings-derived features were grouped.
The last two categories were composed of the NetSurfP-2 predicted torsion angles, and of the relative and absolute NetSurfP-2 predicted solvent accessibility.

For each feature group, the following procedure was used for evaluating the permutation importance.
First, the baseline performance $\hat{p}$ of the model on the test set was determined, as explained in the sections relative to each of the models.
Then, one feature group at a time, the columns corresponding to that group of features in the test set were randomly permutated.
The same model was used again to predict the fitness scores using this corrupted feature set, measuring a group-specific performance $p_{jk}$ for feature group $j$.
The process was repeated $K=5$ times for each feature group using different random permutations of the columns.
Each repetition is referred to with the subscript $k$ in this notation.
The importance $i_j$ for feature group $j$ was then evaluated according to equation \autoref{eq:permutation_importance}.

\begin{equation}\label{eq:permutation_importance}
	i_j = \hat{p} - \frac{1}{K} \sum_{k=1}^K p_{jk}
\end{equation}

In words, the performance on the corrupted dataset for a given feature group across the \num{5} repetitions was averaged and the average was subtracted from the original score of the model.
Thus, a feature importance of \num{0} indicates that the feature group is irrelevant for the model since when the values of the respective features are randomized the performance of the model is not altered.
An importance score equal to the performance of the original model indicates that the model entirely depends on that feature group since when it is randomized the performance drops to \num{0}.
A negative importance score indicates that the model performs better than the baseline when the feature group is randomized.
The feature importance was evaluated independently for each of the nine datasets used in this work.
