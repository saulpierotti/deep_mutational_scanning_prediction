{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "veterinary-stable",
   "metadata": {},
   "source": [
    "# Deep learning training dataset creation Gray2018\n",
    "\n",
    "My general approach is to give the raw MSA in input to the model and let it extract its own features. Here I compile the data that I will then use for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "difficult-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; pd.set_option('display.max_columns', None)\n",
    "import requests\n",
    "import numpy as np\n",
    "import joblib\n",
    "from Bio import SeqIO, AlignIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-anger",
   "metadata": {},
   "source": [
    "## Retrieving input sequences\n",
    "\n",
    "I exclude the studies that were excluded in the gray2018 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adverse-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/gray2018/dmsTraining_2017-02-20.csv')\n",
    "\n",
    "# some of the studies in the training set were excluded from training in the original paper\n",
    "excluded_studies = ['Brca1_E3', 'Brca1_Y2H', 'E3_ligase']\n",
    "for study in excluded_studies:\n",
    "    df = df[df['dms_id'] != study]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-glance",
   "metadata": {},
   "source": [
    "I fetch the WT sequences in the training set from uniprot and I put them in a single file. I also check that the declared length agrees with the uniprot length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "molecular-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_uri = 'https://www.uniprot.org/uniprot/'\n",
    "out_sequences_path = '../processing/gray2018/sequences/'\n",
    "\n",
    "uniprot_id_set = set(df['uniprot_id'])\n",
    "for uniprot_id in set(df['uniprot_id']):\n",
    "    current_fasta_url = uniprot_uri + uniprot_id + '.fasta'\n",
    "    r = requests.get(current_fasta_url)\n",
    "    fasta_seqence = r.text\n",
    "    fasta_file = out_sequences_path + uniprot_id + '.fasta'\n",
    "    with open(fasta_file, 'w') as handle:\n",
    "        handle.write(fasta_seqence)\n",
    "    \n",
    "    # this is just a consistency check\n",
    "    record = list(SeqIO.parse(fasta_file, \"fasta\"))[0]\n",
    "    declared_protein_len = set(df[df['uniprot_id'] == uniprot_id]['protein_size'])\n",
    "    assert len(declared_protein_len) == 1\n",
    "    assert declared_protein_len.pop() == len(record.seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-controversy",
   "metadata": {},
   "source": [
    "## MSA preparation\n",
    "\n",
    "I decided to use HHblits for building the MSA for each query sequence. As a database I use the pre-fromatted `uniclust30`. I perform 3 iterations  and 0.001 maximum E value. I took these parameters from the rawMSA paper.\n",
    "I use -all to disable the msa filters, since I will apply them later. In the original rawMSA publication they also provided a filter `-diff inf` but this is not needed since for values of 0 or NaN the filter is inactive, and 0 is the default. `-diff n` retains only the n most diverse sequences in the alignment. This is done so that each alignment block of 50 positions has at least n sequences covering it.\n",
    "\n",
    "```\n",
    "hhblits -i <input_seq> -o <result_file> -d <database> -oa3m <out_msa_file> -n 3 -e 0.001 -all\n",
    "```\n",
    "\n",
    "I filter for sequences with 50% minimum coverage to the query and 99% maximum pairwise identity.\n",
    "The coverage and identity filters are applied later on the resulting MSA using `hhfilter`, so that I can try different parameters without running again hhblits.\n",
    "\n",
    "```\n",
    "hhfilter -i <in_msa_file> -o <out_msa_file> -id 99 -cov 50\n",
    "```\n",
    "\n",
    "I can reformat the a3m alignments to fasta using a script in the hh suite.  In the rawMSA paper the authors also remove the insertions relative to the query by filtering out the lowercase letters in the a3m file. I do the same with the reformat script of the hh-suite by specifying the `-r` parameter.\n",
    "\n",
    "```\n",
    "reformat.pl -r a3m fas <infile> <outfile>\n",
    "```\n",
    "\n",
    "Now that I have the fasta msa, I need to convert it to an integer representation for the keras embedding that I will then apply to it. I am saving the output as a numpy array. I do not filter by depth here, that will be done later to the numpy array at training.\n",
    "\n",
    "## MSA visualization\n",
    "\n",
    "I use aliview for the visualization of the MSAs, but there are too many sequences to be seen confortably.\n",
    "I create 2 different sets for viewing. One is a representative 30 sequences of the diversity in the MSA.\n",
    "\n",
    "```\n",
    "hhfilter -i <in_msa_file> -o <out_msa_file> -diff 30\n",
    "```\n",
    "\n",
    "The other is the top1000 sequences, that are supposed to be the most similar to the query and the ones that I will end up actually using in the prediction. I use the seqkit script.\n",
    "\n",
    "```\n",
    "seqkit head -n 1000 <in_msa_file> > <out_msa_file>\n",
    "```\n",
    "\n",
    "These edited alignments can be visualised with aliview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-topic",
   "metadata": {},
   "source": [
    "## Vectorizing the MSA\n",
    "\n",
    "I convert the full MSA to a vector where each symbol is assigned to an integer. I do so since I will use an embedding layer in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "endless-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P00552.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P02829.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P04147.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P06654.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P0CG48.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P0CG63.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P31016.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P46937.fasta\n",
      "Processing file: ../processing/gray2018/hhblits_msa_filtered_noinsert/P62593.fasta\n"
     ]
    }
   ],
   "source": [
    "id_list = open('../processing/gray2018/input_list.txt')\n",
    "msa_path = '../processing/gray2018/hhblits_msa_filtered_noinsert/'\n",
    "out_vec_path = '../processing/gray2018/msa_vectors/'\n",
    "possible_chars = 'ARNDCQEGHILKMFPSTWYV-XBZU'\n",
    "\n",
    "for line in id_list:\n",
    "    protein = line.rstrip()\n",
    "    msa_file = msa_path + protein + '.fasta'\n",
    "    first_seq = True\n",
    "    msa = AlignIO.read(msa_file, 'fasta')\n",
    "    out_vec = []\n",
    "    print('Processing file:', msa_file)\n",
    "    for record in msa:\n",
    "        if first_seq:\n",
    "            first_seq = False\n",
    "            first_seq_id = record.id.split('|')[1]\n",
    "            assert first_seq_id == protein\n",
    "        sequence = record.seq.upper()\n",
    "        for char in set(sequence):\n",
    "            assert char in possible_chars\n",
    "        # I add 1 since 0 is reserved for padding in the embedding input\n",
    "        seq_mapped = [possible_chars.index(char) + 1 for char in sequence]\n",
    "        out_vec.append(seq_mapped)\n",
    "    out_vec = np.array(out_vec)\n",
    "    assert out_vec.shape == (len(msa), msa.get_alignment_length())\n",
    "    joblib.dump(out_vec, out_vec_path + protein + '.npy.joblib.xz', compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-parallel",
   "metadata": {},
   "source": [
    "I could process now sliding windows for each mutation, but I want first to have a look at the literature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
