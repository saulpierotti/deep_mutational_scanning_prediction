{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "russian-transportation",
   "metadata": {},
   "source": [
    "# Deep learning Gray2018 full msa approach\n",
    "\n",
    "Here I implement the network used to process the MSA obtained from the filtered output of hhblits. This implementation uses the network that extracts the contact map from an msa, and then further processes the output from there with additional layers. This approach considers the full msa and NOT a sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intimate-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-evening",
   "metadata": {},
   "source": [
    "## Original rawMSA cmap network\n",
    "\n",
    "Here I re-create the model architecture of the rawMSA cmap network. There is no source code in the repository of the rawMSA paper, and the serialised model do not de-serialize correctly. A bad marshal error is raised at loading time, and I determined this to be due to a mismatch in python version that is incompatible with the bytecode of the lambda layer. The models can be successfully loaded under python 3.5. I loaded them in such way and then save the model architecture and weights separately for easier processsing. The architecture was saved a s a json.\n",
    "Here I code the same architecture to use as a base for my model. The lambda layer was recreated by reading the paper and interpreting the operations that they did, so it may differ in the actual implementation. The layer naming does not coincide since the first layeris called layer_1 in the original cmap and layer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "working-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my input is of shape LY where L is the lenght of the MSA (variable for each input) and Y is the depth (an hyperparameter)\n",
    "# In this original implementation the input is a vector, so I presume that they first flatten the msa to give as input\n",
    "# this is not really declared anywhere but if I give in input a None,None shape, then the reshape removes a dimension!\n",
    "# the dimensions declared here do not include the batch size, which is added as a None first dimension automatically\n",
    "# note that this is a tensor shape, not an input layer\n",
    "# for the functional API is recommended in place of InputLayer\n",
    "inputs = tf.keras.Input(shape=(None,))\n",
    "# this is the embedding layer\n",
    "# 26 is the number of residues (20 standard, the additional characthers XBZU, and - for gaps)\n",
    "# 28 is the dimensionality of the embedding used\n",
    "# I am avoiding here to specify many parameters, if the defaults differ from what is in the rawMSA paper I will adjust it later\n",
    "# x is the running variable that I use to connect the layers\n",
    "# the shape after the embedding is batch,LY,E\n",
    "embedding = tf.keras.layers.Embedding(input_dim=26, output_dim=28)\n",
    "x = embedding(inputs)\n",
    "# I reshape to undo the initial flattening of the input, so I separate the L and Y dimensions (length and alignment depth)\n",
    "# my shape now is batch,L,Y,E\n",
    "reshape = tf.keras.layers.Reshape((-1,1000,28))\n",
    "x = reshape(x)\n",
    "# here I start the first round of convolutions and pooling\n",
    "# I declare the layers inside the loop since otherwise it throws an error on the shape of the input for conv2d\n",
    "# probably some internal param is initialized and remains so during subsequent calls\n",
    "# recreating the layer each time just resets everything\n",
    "# In order to avoid possible hard-to-debug problems I re-initialize also activation and batch_norm\n",
    "# the general structure is a block of 2 convolutions and batch normalization followed by max pooling\n",
    "# this block is repeated 4 to 8 times (in this case 6)\n",
    "# after the first pass the third dimension goes from 28 to 22 and then remains constant\n",
    "# from now on the third dimension will be the filter dimension F instead of the embedding dimension E\n",
    "# the second dimension will be the stride dimension S instead than the msa depth dimension Y\n",
    "# the activation and batch normalization do not affect the shape\n",
    "# max pooling halves the S dimension at each round\n",
    "for _ in range(6):\n",
    "    for _ in range(2):\n",
    "        # I use 22 convolutional filters of size 1,3 with same padding\n",
    "        # The channel (the F dimension) is the last\n",
    "        conv2d = tf.keras.layers.Conv2D(22, [1,3], padding='same', data_format='channels_last')\n",
    "        x = conv2d(x)\n",
    "        # apply a relu on the conv2d output\n",
    "        # I am not sure why they did not just apply the activation in the conv2d layer\n",
    "        activation = tf.keras.layers.Activation('relu')\n",
    "        x = activation(x)\n",
    "        # batch normalization brings the mean output close to 0 and the sd close to 1\n",
    "        # during training nornmalization is done on the current batch\n",
    "        # during testing, it normalizes according to the training set\n",
    "        batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "        x = batch_normalization(x)\n",
    "    # the final max pooling layer\n",
    "    # this layer halves the S dimension at each round\n",
    "    max_pooling = tf.keras.layers.MaxPooling2D(pool_size=[1,2], data_format='channels_last')\n",
    "    x = max_pooling(x)\n",
    "# after the firt convolutions and pooling loop, the shape is batch,L,Y/n,22\n",
    "# n is the number of max pooling layers applied, and the division is approximated to the closes integer down\n",
    "# for n=6 the shape is batch,L,15,22\n",
    "# this reshape concatenates the S and F axis, removing one dimension (15*22=330)\n",
    "# after the reshape the shape is batch,L,330\n",
    "reshape = tf.keras.layers.Reshape((-1,330))\n",
    "x = reshape(x)\n",
    "# the lambda layer for the outer product\n",
    "# this layer has the purpose of converting the dimensionality closer to that of the output (L,L)\n",
    "# the input here has shape batch,L,330 where 330 is (F*S)\n",
    "# the output has shape batch,L,L,330\n",
    "# first I define the function to compute the outer product\n",
    "def outer_product(x):\n",
    "    # actually I need only FS but I put the correct names for reference\n",
    "    batch, L, FS = x.shape\n",
    "    # x_hat and x_bar have both a singleton dimension added at different orders (before and after L)\n",
    "    x_hat = tf.keras.layers.Reshape((-1, 1, FS))(x)\n",
    "    x_bar = tf.keras.layers.Reshape((1, -1, FS))(x)\n",
    "    # the multiply operator returns the element-wise multiplication\n",
    "    # since there is a dimensional mismatch, the singleton dimensions are bradcasted to the opposite L dimension\n",
    "    x = tf.math.multiply(x_hat, x_bar)\n",
    "    return x\n",
    "# then I create the lambda layer implementing the custom outer product function\n",
    "lambda_layer = tf.keras.layers.Lambda(outer_product, output_shape=[None, None, 330])\n",
    "x = lambda_layer(x)\n",
    "# now the second and final round of convolutions\n",
    "# the first 3 layers are non-repetitive\n",
    "# I start with a convolution with 34 filters of shape 3,3 and with relu activation\n",
    "# This brings the shape from batch,L,L,330 (where 330 is the old FS) to batch,L,L,34 where 34 is the new F\n",
    "conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "x = conv2d(x)\n",
    "# the batch normalization is identical to the one in the initial loop and does not alter the shape\n",
    "batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "x = batch_normalization(x)\n",
    "# this is similar to the previous conv2d but with a linear activation instead of relu\n",
    "# this is done since the relu activation is the first component of the subsequent loop\n",
    "# here the shape is not altered (batch,L,L,F=34) since I am using the same number of filters\n",
    "conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='linear', padding='same', data_format='channels_last')\n",
    "x = conv2d(x)\n",
    "# I have now a loop of relu activation followed by 2 rounds of batch normalization and conv2d, which ends\n",
    "# with adding the output of the last convolution to the output of the first batch norm\n",
    "# the loop is repeated 15 times (6 to 20 times in different models)\n",
    "for _ in range(15):\n",
    "    # the activation is identical to the one in the initial loop and does not alter the shape (batch,L,L,F=34)\n",
    "    activation = tf.keras.layers.Activation('relu')\n",
    "    x = activation(x)\n",
    "    # batch norm and conv2d are repeated twice per each block\n",
    "    # I nonetheless state them explicitly since I need a skip connection from the first batch norm\n",
    "    # the batch normalization is identical to the one in the initial loop\n",
    "    # I name x differently here since I need to retrieve the output for the skip connection\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "    x_skip = batch_normalization(x)\n",
    "    # convolution with 34 filters of shape 3,3 and with relu activation\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "    x = conv2d(x_skip)\n",
    "    # the batch normalization is identical to the one in the initial loop\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "    x = batch_normalization(x)\n",
    "    # convolution with 34 filters of shape 3,3 and with relu activation\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "    x = conv2d(x)\n",
    "    # this a skip connection: I add to the current tensor x the output of the FIRST batch norm of the loop\n",
    "    # the 2 tensors have the same shape and also the output has the same shape of batch,L,L,FS=34\n",
    "    add = tf.keras.layers.Add()\n",
    "    x = add([x, x_skip])\n",
    "# the network finishes with a convolution preceeded by normalization and activation\n",
    "# the activation is identical to the one in the initial loop and does not alter the shape (batch,L,L,F=34)\n",
    "activation = tf.keras.layers.Activation('relu')\n",
    "x = activation(x)    \n",
    "# the batch normalization is identical to the one in the initial loop\n",
    "# this does not alter the shape (batch,L,L,F=34)\n",
    "batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "x = batch_normalization(x)\n",
    "# the final convolution has only 2 filters with size 3,3\n",
    "# this reduces the F dimension to 2 bringing the shape to batch,L,L,F=2\n",
    "conv2d = tf.keras.layers.Conv2D(2, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "x = conv2d(x)\n",
    "# the final activation is a softmax and does not alter the shape (batch,L,L,F=2)\n",
    "# this normalizes the output to a probability such that the 2 channels measure the probability of contact/no contact\n",
    "activation = tf.keras.layers.Activation('softmax')\n",
    "outputs = activation(x) \n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cmap_recreated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-colonial",
   "metadata": {},
   "source": [
    "## My derivative of the rawMSA cmap network\n",
    "\n",
    "I start now from the original cmap net and I modify it for my needs. I find the way the input is fed counter-intuitive, so I make the input be of shape L,Y instead of LY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my input is of shape L,Y where L is the lenght of the MSA (variable for each input) and Y is the depth (an hyperparameter)\n",
    "# I do not declare Y for generality\n",
    "# the dimensions declared here do not include the batch size, which is added as a None first dimension automatically\n",
    "# note that this is a tensor shape, not an input layer\n",
    "# for the functional API is recommended in place of InputLayer\n",
    "inputs = tf.keras.Input(shape=(None,1000))\n",
    "# this flattening was added by me to do the flattening implicitly and not before feeding the network\n",
    "# flattening is required for the embedding layer\n",
    "# I am converting from batch,L,Y to batch,LY\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "x = flatten(inputs)\n",
    "# this is the embedding layer\n",
    "# 26 is the number of residues (20 standard, the additional characthers XBZU, and - for gaps)\n",
    "# 28 is the dimensionality of the embedding used\n",
    "# I am avoiding here to specify many parameters, if the defaults differ from what is in the rawMSA paper I will adjust it later\n",
    "# x is the running variable that I use to connect the layers\n",
    "# the shape after the embedding is batch,LY,E\n",
    "embedding = tf.keras.layers.Embedding(input_dim=26, output_dim=28)\n",
    "x = embedding(x)\n",
    "# I do a reshape to recreate the Y dimension (alignment depth)\n",
    "# my shape now is batch,L,Y,E\n",
    "reshape = tf.keras.layers.Reshape((-1,1000,28))\n",
    "x = reshape(x)\n",
    "# here I start the first round of convolutions and pooling\n",
    "# I declare the layers inside the loop since otherwise it throws an error on the shape of the input for conv2d\n",
    "# probably some internal param is initialized and remains so during subsequent calls\n",
    "# recreating the layer each time just resets everything\n",
    "# In order to avoid possible hard-to-debug problems I re-initialize also activation and batch_norm\n",
    "# the general structure is a block of 2 convolutions and batch normalization followed by max pooling\n",
    "# this block is repeated 4 to 8 times (in this case 6)\n",
    "# after the first pass the third dimension goes from 28 to 22 and then remains constant\n",
    "# from now on the third dimension will be the filter dimension F instead of the embedding dimension E\n",
    "# the second dimension will be the stride dimension S instead than the msa depth dimension Y\n",
    "# the activation and batch normalization do not affect the shape\n",
    "# max pooling halves the S dimension at each round\n",
    "for _ in range(6):\n",
    "    for _ in range(2):\n",
    "        # I use 22 convolutional filters of size 1,3 with same padding\n",
    "        # The channel (the F dimension) is the last\n",
    "        conv2d = tf.keras.layers.Conv2D(22, [1,3], padding='same', data_format='channels_last')\n",
    "        x = conv2d(x)\n",
    "        # apply a relu on the conv2d output\n",
    "        # I am not sure why they did not just apply the activation in the conv2d layer\n",
    "        activation = tf.keras.layers.Activation('relu')\n",
    "        x = activation(x)\n",
    "        # batch normalization brings the mean output close to 0 and the sd close to 1\n",
    "        # during training nornmalization is done on the current batch\n",
    "        # during testing, it normalizes according to the training set\n",
    "        batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "        x = batch_normalization(x)\n",
    "    # the final max pooling layer\n",
    "    # this layer halves the S dimension at each round\n",
    "    max_pooling = tf.keras.layers.MaxPooling2D(pool_size=[1,2], data_format='channels_last')\n",
    "    x = max_pooling(x)\n",
    "# after the firt convolutions and pooling loop, the shape is batch,L,Y/n,22\n",
    "# n is the number of max pooling layers applied, and the division is approximated to the closes integer down\n",
    "# for n=6 the shape is batch,L,15,22\n",
    "# this reshape concatenates the S and F axis, removing one dimension (15*22=330)\n",
    "# after the reshape the shape is batch,L,330\n",
    "reshape = tf.keras.layers.Reshape((-1,330))\n",
    "x = reshape(x)\n",
    "# the lambda layer for the outer product\n",
    "# this layer has the purpose of converting the dimensionality closer to that of the output (L,L)\n",
    "# the input here has shape batch,L,330 where 330 is (F*S)\n",
    "# the output has shape batch,L,L,330\n",
    "# first I define the function to compute the outer product\n",
    "def outer_product(x):\n",
    "    # actually I need only FS but I put the correct names for reference\n",
    "    batch, L, FS = x.shape\n",
    "    # x_hat and x_bar have both a singleton dimension added at different orders (before and after L)\n",
    "    x_hat = tf.keras.layers.Reshape((-1, 1, FS))(x)\n",
    "    x_bar = tf.keras.layers.Reshape((1, -1, FS))(x)\n",
    "    # the multiply operator returns the element-wise multiplication\n",
    "    # since there is a dimensional mismatch, the singleton dimensions are bradcasted to the opposite L dimension\n",
    "    x = tf.math.multiply(x_hat, x_bar)\n",
    "    return x\n",
    "# then I create the lambda layer implementing the custom outer product function\n",
    "lambda_layer = tf.keras.layers.Lambda(outer_product, output_shape=[None, None, 330])\n",
    "x = lambda_layer(x)\n",
    "# now the second and final round of convolutions\n",
    "# the first 3 layers are non-repetitive\n",
    "# I start with a convolution with 34 filters of shape 3,3 and with relu activation\n",
    "# This brings the shape from batch,L,L,330 (where 330 is the old FS) to batch,L,L,34 where 34 is the new F\n",
    "conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "x = conv2d(x)\n",
    "# the batch normalization is identical to the one in the initial loop and does not alter the shape\n",
    "batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "x = batch_normalization(x)\n",
    "# this is similar to the previous conv2d but with a linear activation instead of relu\n",
    "# this is done since the relu activation is the first component of the subsequent loop\n",
    "# here the shape is not altered (batch,L,L,F=34) since I am using the same number of filters\n",
    "conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='linear', padding='same', data_format='channels_last')\n",
    "x = conv2d(x)\n",
    "# I have now a loop of relu activation followed by 2 rounds of batch normalization and conv2d, which ends\n",
    "# with adding the output of the last convolution to the output of the first batch norm\n",
    "# the loop is repeated 15 times (6 to 20 times in different models)\n",
    "for _ in range(15):\n",
    "    # the activation is identical to the one in the initial loop and does not alter the shape (batch,L,L,F=34)\n",
    "    activation = tf.keras.layers.Activation('relu')\n",
    "    x = activation(x)\n",
    "    # batch norm and conv2d are repeated twice per each block\n",
    "    # I nonetheless state them explicitly since I need a skip connection from the first batch norm\n",
    "    # the batch normalization is identical to the one in the initial loop\n",
    "    # I name x differently here since I need to retrieve the output for the skip connection\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "    x_skip = batch_normalization(x)\n",
    "    # convolution with 34 filters of shape 3,3 and with relu activation\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "    x = conv2d(x_skip)\n",
    "    # the batch normalization is identical to the one in the initial loop\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "    x = batch_normalization(x)\n",
    "    # convolution with 34 filters of shape 3,3 and with relu activation\n",
    "    # this does not alter the shape (batch,L,L,F=34)\n",
    "    conv2d = tf.keras.layers.Conv2D(34, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "    x = conv2d(x)\n",
    "    # this a skip connection: I add to the current tensor x the output of the FIRST batch norm of the loop\n",
    "    # the 2 tensors have the same shape and also the output has the same shape of batch,L,L,FS=34\n",
    "    add = tf.keras.layers.Add()\n",
    "    x = add([x, x_skip])\n",
    "# the network finishes with a convolution preceeded by normalization and activation\n",
    "# the activation is identical to the one in the initial loop and does not alter the shape (batch,L,L,F=34)\n",
    "activation = tf.keras.layers.Activation('relu')\n",
    "x = activation(x)    \n",
    "# the batch normalization is identical to the one in the initial loop\n",
    "# this does not alter the shape (batch,L,L,F=34)\n",
    "batch_normalization = tf.keras.layers.BatchNormalization(axis=[3])\n",
    "x = batch_normalization(x)\n",
    "# the final convolution has only 2 filters with size 3,3\n",
    "# this reduces the F dimension to 2 bringing the shape to batch,L,L,F=2\n",
    "conv2d = tf.keras.layers.Conv2D(2, [3,3], activation='relu', padding='same', data_format='channels_last')\n",
    "x = conv2d(x)\n",
    "# the final activation is a softmax and does not alter the shape (batch,L,L,F=2)\n",
    "# this normalizes the output to a probability such that the 2 channels measure the probability of contact/no contact\n",
    "activation = tf.keras.layers.Activation('softmax')\n",
    "outputs = activation(x) \n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "great-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, 1000)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, None)         0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 28)     728         flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, None, 1000, 2 0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, None, 1000, 2 1870        reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, None, 1000, 2 0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, 1000, 2 88          activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, None, 1000, 2 1474        batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, None, 1000, 2 0           conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, 1000, 2 88          activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, None, 500, 22 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, None, 500, 22 1474        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, None, 500, 22 0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, 500, 22 88          activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, None, 500, 22 1474        batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, None, 500, 22 0           conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, 500, 22 88          activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, None, 250, 22 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, None, 250, 22 1474        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, None, 250, 22 0           conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, 250, 22 88          activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, None, 250, 22 1474        batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, None, 250, 22 0           conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, 250, 22 88          activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, None, 125, 22 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, None, 125, 22 1474        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, None, 125, 22 0           conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, 125, 22 88          activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, None, 125, 22 1474        batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, None, 125, 22 0           conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, 125, 22 88          activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, None, 62, 22) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, None, 62, 22) 1474        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, None, 62, 22) 0           conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, 62, 22) 88          activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, None, 62, 22) 1474        batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, None, 62, 22) 0           conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, 62, 22) 88          activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, None, 31, 22) 0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, None, 31, 22) 1474        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, None, 31, 22) 0           conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, 31, 22) 88          activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, None, 31, 22) 1474        batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, None, 31, 22) 0           conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, 31, 22) 88          activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, None, 15, 22) 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, None, 330)    0           max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, None, 3 0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, None, None, 3 101014      lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, None, 3 136         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, None, None, 3 10438       batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, None, None, 3 0           conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, None, 3 136         activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, None, None, 3 10438       batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, None, 3 136         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, None, None, 3 10438       batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, None, None, 3 0           conv2d_105[0][0]                 \n",
      "                                                                 batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, None, None, 3 0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, None, 3 136         activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, None, None, 3 10438       batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, None, 3 136         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, None, None, 3 10438       batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, None, None, 3 0           conv2d_107[0][0]                 \n",
      "                                                                 batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, None, None, 3 0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, None, 3 136         activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, None, None, 3 10438       batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, None, 3 136         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, None, None, 3 10438       batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, None, None, 3 0           conv2d_109[0][0]                 \n",
      "                                                                 batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, None, None, 3 0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, None, 3 136         activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, None, None, 3 10438       batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, None, 3 136         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, None, None, 3 10438       batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, None, None, 3 0           conv2d_111[0][0]                 \n",
      "                                                                 batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, None, None, 3 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, None, 3 136         activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, None, None, 3 10438       batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, None, 3 136         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, None, None, 3 10438       batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, None, None, 3 0           conv2d_113[0][0]                 \n",
      "                                                                 batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, None, None, 3 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, None, 3 136         activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, None, None, 3 10438       batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, None, 3 136         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, None, None, 3 10438       batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, None, None, 3 0           conv2d_115[0][0]                 \n",
      "                                                                 batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, None, None, 3 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, None, 3 136         activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, None, None, 3 10438       batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, None, 3 136         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, None, None, 3 10438       batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, None, None, 3 0           conv2d_117[0][0]                 \n",
      "                                                                 batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, None, None, 3 0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, None, None, 3 136         activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, None, None, 3 10438       batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, None, 3 136         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, None, None, 3 10438       batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, None, None, 3 0           conv2d_119[0][0]                 \n",
      "                                                                 batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, None, None, 3 0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, None, 3 136         activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, None, None, 3 10438       batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, None, 3 136         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, None, None, 3 10438       batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, None, None, 3 0           conv2d_121[0][0]                 \n",
      "                                                                 batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, None, None, 3 0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, None, 3 136         activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, None, None, 3 10438       batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, None, 3 136         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, None, None, 3 10438       batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, None, None, 3 0           conv2d_123[0][0]                 \n",
      "                                                                 batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, None, None, 3 0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, None, 3 136         activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, None, None, 3 10438       batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, None, 3 136         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, None, None, 3 10438       batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, None, None, 3 0           conv2d_125[0][0]                 \n",
      "                                                                 batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, None, None, 3 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, None, 3 136         activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, None, None, 3 10438       batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, None, 3 136         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, None, None, 3 10438       batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, None, None, 3 0           conv2d_127[0][0]                 \n",
      "                                                                 batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, None, None, 3 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, None, 3 136         activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, None, None, 3 10438       batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, None, 3 136         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, None, None, 3 10438       batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, None, None, 3 0           conv2d_129[0][0]                 \n",
      "                                                                 batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, None, None, 3 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, None, 3 136         activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, None, None, 3 10438       batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, None, 3 136         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, None, None, 3 10438       batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, None, None, 3 0           conv2d_131[0][0]                 \n",
      "                                                                 batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, None, None, 3 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, None, 3 136         activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, None, None, 3 10438       batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, None, 3 136         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, None, None, 3 10438       batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, None, None, 3 0           conv2d_133[0][0]                 \n",
      "                                                                 batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, None, 3 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, None, 3 136         activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, None, None, 2 614         batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, None, 2 0           conv2d_134[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 449,426\n",
      "Trainable params: 446,722\n",
      "Non-trainable params: 2,704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "removable-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "with open('../temp/my_cmap_summary.txt', 'w') as handle:\n",
    "    with redirect_stdout(handle):\n",
    "        model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atomic-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_json = json.loads(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clean-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_model_json['config']['layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "refined-indian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../processing/raw_msa/cmap_sample_model/architecture.json') as handle:\n",
    "    cmap_model_json = json.load(handle)\n",
    "\n",
    "len(cmap_model_json['config']['layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "reasonable-taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['config']\n",
      " ['name']\n",
      " ['class_name']\n",
      " ['inbound_nodes']]\n",
      "[['class_name']\n",
      " ['config']\n",
      " ['name']\n",
      " ['inbound_nodes']]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([el for el in cmap_model_json['config']['layers'][0]]).reshape(-1,1))\n",
    "print(np.array([el for el in my_model_json['config']['layers'][0]]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "suffering-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference found: config:name \n",
      "old: reshape_1 \tnew: reshape\n",
      "Difference found: name \n",
      "old: reshape_1 \tnew: reshape\n",
      "Difference found: inbound_nodes \n",
      "old: [[['embedding_1', 0, 0, {}]]] \tnew: [[['embedding', 0, 0, {}]]]\n"
     ]
    }
   ],
   "source": [
    "new = my_model_json['config']['layers'][2]\n",
    "old = cmap_model_json['config']['layers'][2]\n",
    "\n",
    "def recursive_differences(old, new, path=''):\n",
    "    if isinstance(old, list):\n",
    "        for i,el in enumerate(old):\n",
    "            if isinstance(old[i], dict) or isinstance(old[i], list):\n",
    "                recursive_differences(old[i], new[i], path + '[' + str(i) + ']:')\n",
    "            elif old[i] != new[i]:\n",
    "                print('Difference found:', path + '[' + str(i) + ']:', '\\nold:', old[i], '\\tnew:', new[i])\n",
    "    elif isinstance(old, dict):\n",
    "        for key in old:\n",
    "            try:\n",
    "                if isinstance(old[key], dict):\n",
    "                    recursive_differences(old[key], new[key], path + key + ':')\n",
    "                elif old[key] != new[key]:\n",
    "                    print('Difference found:', path + key, '\\nold:', old[key], '\\tnew:', new[key])\n",
    "            except KeyError:\n",
    "                print('KeyError at:', path + key, '\\nold:', old[key])\n",
    "            \n",
    "recursive_differences(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "plain-scenario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'ragged': False,\n",
       "  'dtype': 'float32',\n",
       "  'name': 'input_1',\n",
       "  'batch_input_shape': [None, None],\n",
       "  'sparse': False},\n",
       " 'name': 'input_1',\n",
       " 'class_name': 'InputLayer',\n",
       " 'inbound_nodes': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap_model_json['config']['layers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caring-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_name': 'InputLayer',\n",
       " 'config': {'batch_input_shape': [None, None, None],\n",
       "  'dtype': 'float32',\n",
       "  'sparse': False,\n",
       "  'ragged': False,\n",
       "  'name': 'input_1'},\n",
       " 'name': 'input_1',\n",
       " 'inbound_nodes': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_json['config']['layers'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-europe",
   "metadata": {},
   "source": [
    "I first define the possible values with which the input layer has been coded, in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "great-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_chars = 'ARNDCQEGHILKMFPSTWYV-XBZU'\n",
    "input_dim = len(possible_chars) + 1 # I add 1 since 0 is a placeholder for padding\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-master",
   "metadata": {},
   "source": [
    "I load a sample input vector. I still did not define an output y. For now I am filtering with a depth of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rising-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4502, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_vec = joblib.load('../processing/gray2018/hhblits_msa_filtered_vectors/P00552.npy.joblib.xz')\n",
    "msa_depth = 1000\n",
    "x = msa_vec[:msa_depth].T # the original input is L*Y where Y is depth, so I do the same\n",
    "del msa_vec\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-overview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
