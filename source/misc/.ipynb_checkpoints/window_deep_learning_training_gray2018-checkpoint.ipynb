{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-extreme",
   "metadata": {},
   "source": [
    "# Deep learning Gray2018 window approach\n",
    "\n",
    "Here I implement the network used to process the MSA obtained from the filtered output of hhblits. This implementation uses the network that extracts the secondary structure and relative solvent accessibility from an msa, and then further processes the output from there with additional layers. This approach is based on a sliding window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-there",
   "metadata": {},
   "source": [
    "## Obtaining the sliding windows\n",
    "\n",
    "Here I put the sliding windows for each protein and for each position in a dictionary for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "placed-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../processing/gray2018/deep_learning/sliding_windows_unmapped.joblib.xz']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "in_msa_path = '../processing/gray2018/deep_learning/msa_vectors/'\n",
    "basename_list = '../processing/gray2018/input_list.txt'\n",
    "out_sliding_windows_path = '../processing/gray2018/deep_learning/sliding_windows_unmapped.joblib.xz'\n",
    "window_size = 31\n",
    "msa_depth = 500\n",
    "\n",
    "with open(basename_list) as handle:\n",
    "    sliding_windows = {}\n",
    "    for line in handle:\n",
    "        basename = line.rstrip()\n",
    "        msa_vec = joblib.load(in_msa_path + basename + '.npy.joblib.xz')[:msa_depth]\n",
    "        windows_list = []\n",
    "        for i, _ in enumerate(msa_vec.T):\n",
    "            upper = i + ((window_size - 1)//2) + 1\n",
    "            lower = i - ((window_size - 1)//2)\n",
    "            pad_lower, pad_upper = 0, 0\n",
    "            if lower < 0:\n",
    "                pad_lower = - lower\n",
    "                lower = 0\n",
    "            if upper > len(msa_vec.T):\n",
    "                pad_upper = upper - len(msa_vec.T)\n",
    "                # no need to reset upper since numpy allows indeces exceeding len\n",
    "            curr_window_unpadded = msa_vec[:, lower:upper]\n",
    "            # this is for the vertical padding if there are not enough sequences in the msa\n",
    "            pad_vertical = 0\n",
    "            if len(msa_vec) < msa_depth:\n",
    "                pad_vertical = msa_depth - len(msa_vec)\n",
    "            # 0 is a special padding value for the keras embedding layer\n",
    "            curr_window = np.pad(curr_window_unpadded, ((0,pad_vertical),(pad_lower,pad_upper)), mode='constant', constant_values = 0)\n",
    "            assert curr_window.shape == (msa_depth, window_size)\n",
    "            windows_list.append(curr_window)\n",
    "        sliding_windows[basename] = np.array(windows_list)\n",
    "        assert sliding_windows[basename].shape == (len(msa_vec.T), msa_depth, window_size)\n",
    "\n",
    "joblib.dump(sliding_windows, out_sliding_windows_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-maryland",
   "metadata": {},
   "source": [
    "## Purge the dataset from wrong mappings\n",
    "\n",
    "I cross-check that aa1 is correct with the uniprot sequences. I remove entries with a wrong mapping.\n",
    "\n",
    "Noticeable points:\n",
    "- P62593 has some position with a wrong mapping (most probably slided by 2)\n",
    "- P06654 has consistently a Q in position 228 while uniprot reports a T. The MSA reports consistently T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed entries for wrong mapping: 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../processing/gray2018/deep_learning/dms_ids.npy.joblib.xz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "df = pd.read_csv('../dataset/gray2018/dmsTraining_2017-02-20.csv')\n",
    "\n",
    "# some of the studies in the training set were excluded from training in the original paper\n",
    "# beta lactamase I am adding just now for remobving int temporarely\n",
    "excluded_studies = ['Brca1_E3', 'Brca1_Y2H', 'E3_ligase']\n",
    "for study in excluded_studies:\n",
    "    df = df[df['dms_id'] != study]\n",
    "\n",
    "# obtain a dictionary with the uniprot sequences for each protein\n",
    "sequences = {}\n",
    "for basename in set(df.uniprot_id):\n",
    "    curr_seq = str(list(SeqIO.parse('../processing/gray2018/deep_learning/sequences/' + basename + '.fasta', 'fasta'))[0].seq)\n",
    "    sequences[basename] = curr_seq\n",
    "\n",
    "len_before = len(df)\n",
    "    \n",
    "# remove inconsistencies\n",
    "index_in_range = pd.Series([row.position - 1 < len(sequences[row.uniprot_id]) for _, row in df.iterrows()], index=df.index)\n",
    "df = df[index_in_range]\n",
    "correct_aa1 = pd.Series([row.aa1 == sequences[row.uniprot_id][row.position - 1] for _, row in df.iterrows()], index=df.index)\n",
    "df = df[correct_aa1]\n",
    "\n",
    "# check that there are no more inconsistencies\n",
    "for protein in set(df.uniprot_id.values):\n",
    "    curr_df = df[df.uniprot_id == protein]\n",
    "    for position in set(curr_df.position.values):\n",
    "        aa1_set = set(curr_df[curr_df.position == position].aa1)\n",
    "        assert len(aa1_set) == 1\n",
    "        curr_aa1 = aa1_set.pop()\n",
    "        seq_index = position - 1\n",
    "        curr_real_res = sequences[protein][seq_index]\n",
    "        assert curr_aa1 == curr_real_res\n",
    "                \n",
    "# print how many entries have been removed (the false values)\n",
    "len_after = len(df)\n",
    "print('Removed entries for wrong mapping:', len_before - len_after)\n",
    "\n",
    "# create the target, protein, and dataset vectors for the edited set (not the x vector since I need to map the sliding windows)\n",
    "joblib.dump(np.array(df.scaled_effect1), '../processing/gray2018/deep_learning/target_fitness.npy.joblib.xz')\n",
    "joblib.dump(np.array(df.protein), '../processing/gray2018/deep_learning/protein.npy.joblib.xz')\n",
    "joblib.dump(np.array(df.dms_id), '../processing/gray2018/deep_learning/dms_ids.npy.joblib.xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-explanation",
   "metadata": {},
   "source": [
    "## Map the sliding windows to the dataset\n",
    "\n",
    "I create an array with the correct sliding window for each dataset position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suitable-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../processing/gray2018/deep_learning/sliding_windows_mapped.npy.joblib.xz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def map_windows(sliding_windows, df):\n",
    "    # the first element of iterrows is the index\n",
    "    windows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # I remove 1 to position since the index starts from 0 but the position from 1\n",
    "        curr_window = sliding_windows[row['uniprot_id']][row['position']-1]\n",
    "        windows.append(curr_window)\n",
    "    windows_vec = np.array(windows)\n",
    "    assert windows_vec.shape[0] == len(df)\n",
    "    return windows_vec\n",
    "\n",
    "\n",
    "sliding_windows = joblib.load('../processing/gray2018/deep_learning/sliding_windows_unmapped.joblib.xz')\n",
    "joblib.dump(map_windows(sliding_windows, df), '../processing/gray2018/deep_learning/sliding_windows_mapped.npy.joblib.xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-thirty",
   "metadata": {},
   "source": [
    "## Create and train the deep learning network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-conviction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521/521 [==============================] - ETA: 0s - loss: 0.3174"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "window_size = 31\n",
    "msa_depth = 500\n",
    "embedding_dim = 14\n",
    "\n",
    "def create_model(window_size=window_size, msa_depth=msa_depth, embedding_dim=embedding_dim):\n",
    "    inputs = tf.keras.Input(shape=(msa_depth, window_size))\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Embedding(input_dim=26, output_dim=embedding_dim, mask_zero=True)(x)\n",
    "    x = tf.keras.layers.Reshape((-1,msa_depth,embedding_dim))(x)\n",
    "    x = tf.keras.layers.Conv2D(embedding_dim, [1,10], activation='relu', padding='same', data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=[1,20], data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Reshape((-1,embedding_dim*(msa_depth//20)))(x)\n",
    "\n",
    "    for _ in range(2):\n",
    "        lstm_layer = tf.keras.layers.LSTM(embedding_dim*(msa_depth//20), return_sequences=True, recurrent_activation='hard_sigmoid')\n",
    "        x = tf.keras.layers.Bidirectional(lstm_layer, merge_mode='ave')(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten(data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='window_dms_predictor')\n",
    "    optimizer = tf.keras.optimizers.RMSprop()\n",
    "    loss = tf.keras.losses.mean_squared_error\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model\n",
    "\n",
    "# load the data vectors\n",
    "x = joblib.load('../processing/gray2018/deep_learning/sliding_windows_mapped.npy.joblib.xz')\n",
    "y = joblib.load('../processing/gray2018/deep_learning/target_fitness.npy.joblib.xz')\n",
    "datasets = joblib.load('../processing/gray2018/deep_learning/dms_ids.npy.joblib.xz')\n",
    "proteins = joblib.load('../processing/gray2018/deep_learning/protein.npy.joblib.xz')\n",
    "#np.random.shuffle(y)\n",
    "\n",
    "# tensorflow logging\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "cv_indexes = LeaveOneGroupOut().split(x, y, proteins)\n",
    "result_vectors = {}\n",
    "\n",
    "for train, val in cv_indexes:\n",
    "    assert len(set(proteins[val])) == 1\n",
    "    curr_protein = set(proteins[val]).pop()\n",
    "    result_vectors[curr_protein] = {}\n",
    "    model = create_model()\n",
    "    model.fit(x[train], y[train], epochs=1, validation_data=(x[val], y[val]), callbacks=[tensorboard_callback])\n",
    "    model.save('../models/window_deep_learning/only_msa_no_mutation/tf2.4_keras_model_' + curr_protein)\n",
    "    # need to flatten since the output has shape (n,1) but I need (n,) for calculating the correlation\n",
    "    y_pred = model.predict(x[train]).flatten()\n",
    "    print(curr_protein, 'training scores')\n",
    "    print('Pearson:', stats.pearsonr(y[train], y_pred)[0], '\\tSpearman:', stats.spearmanr(y[train], y_pred)[0])\n",
    "    del y_pred\n",
    "    for curr_dataset in set(datasets[val]):\n",
    "        y_pred = model.predict(x[val][datasets[val] == curr_dataset]).flatten()\n",
    "        result_vectors[curr_protein][curr_dataset] = (y[val][datasets[val] == curr_dataset], y_pred)\n",
    "        joblib.dump(result_vectors, '../models/window_deep_learning/only_msa_no_mutation/predictions.joblib.xz')\n",
    "        print(curr_protein, curr_dataset, 'validation scores')\n",
    "        print('Pearson:', stats.pearsonr(y[val][datasets[val] == curr_dataset], y_pred)[0],\n",
    "              '\\tSpearman:', stats.spearmanr(y[val][datasets[val] == curr_dataset], y_pred)[0])\n",
    "        plt.close()\n",
    "        sns.scatterplot(x=y[val][datasets[val] == curr_dataset], y=y_pred, s=10)\n",
    "        plt.show()\n",
    "        del y_pred\n",
    "        del curr_dataset\n",
    "    del curr_protein\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-decision",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
