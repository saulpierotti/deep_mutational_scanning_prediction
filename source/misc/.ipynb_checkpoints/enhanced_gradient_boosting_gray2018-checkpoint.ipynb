{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "simple-gnome",
   "metadata": {},
   "source": [
    "# Enhanced gradient boosting Gray2018\n",
    "\n",
    "In this notebook I aim at developping my own version of a gradient boosting algo for predicting the Gray2018 dataset. My focus here is on feature engeneering.\n",
    "\n",
    "## Observations on the features\n",
    "\n",
    "- Phisico-chemical features are not harmful but give avery tiny improvement\n",
    "- Sequence sliding window on the profile does not improve performance consistently (improve for some and loewrs for others)\n",
    "- Shannon entropy is redundant with profile and usually detrimental\n",
    "- aa likelyhood is very useful, but only aa2 and delta, not aa1\n",
    "- netsurf is informative\n",
    "- aa11hot and aa21hot are useful\n",
    "- num_contact is not useful and also long range contacts\n",
    "- profile is very useful\n",
    "- all the features for the top-n contacts are not useful with varying n values\n",
    "- the netsurf features seem all useful\n",
    "    - maybe using only q3 ss and not q8 can be better\n",
    "    - both rsa and asa seem needed\n",
    "\n",
    "## Loading the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "leading-thought",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed entries for wrong mapping or unknown mutation: 378\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "import joblib\n",
    "from Bio import SeqIO\n",
    "\n",
    "df = pd.read_csv('../dataset/gray2018/dmsTraining_2017-02-20.csv')\n",
    "\n",
    "# some of the studies in the training set were excluded from training in the original paper\n",
    "# I also remove here proteins without a structure\n",
    "excluded_studies = ['Brca1_E3', 'Brca1_Y2H', 'E3_ligase']\n",
    "for study in excluded_studies:\n",
    "    df = df[df.dms_id != study]\n",
    "    \n",
    "len_before = len(df)\n",
    "# remove unknown mutations\n",
    "df = df[~(df.aa2 == 'X')]\n",
    "\n",
    "# the ubiquitin studies (Ubiquitin and E1_ubiquitin) are mapped as uniprot id to what is actually a poly-ubiquitin repeat\n",
    "# Also, they are mapped in one case to human and in 1 case to yest\n",
    "# the paper doeas not decleare which sequence was used, but I chose the first repeat of yeast ubiquitin as representative\n",
    "# Here I replace the declared uniprot id with my modified representative sequence\n",
    "df.loc[df.protein == 'UBI4', ['uniprot_id']] = 'P0CG63_1-76'\n",
    "df.loc[df.protein == 'hsp90', ['uniprot_id']] = 'P02829_2-231'\n",
    "                \n",
    "# print how many entries have been removed (the false values)\n",
    "len_after = len(df)\n",
    "print('Removed entries for wrong mapping or unknown mutation:', len_before - len_after)\n",
    "\n",
    "# this df will contain different normalizations of the outputs, and pointers to the protein and dataset of origin\n",
    "effect_df = pd.DataFrame({'protein': df.protein,\n",
    "                          'dms_id': df.dms_id,\n",
    "                          'position': df.position,\n",
    "                          'aa1': df.aa1,\n",
    "                          'aa2': df.aa2,\n",
    "                          'uniprot_id': df.uniprot_id,\n",
    "                          'pdb_id': df.pdb_id,\n",
    "                          'pdb_chain_id': df.pdb_chain_id,\n",
    "                          'reported_fitness': df.reported_fitness,\n",
    "                          'scaled_effect1': df.scaled_effect1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-luxury",
   "metadata": {},
   "source": [
    "## Adding new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "scientific-northwest",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P06654\n",
      "P62593\n",
      "P31016\n",
      "P02829_2-231\n",
      "P46937\n",
      "P0CG63_1-76\n",
      "P00552\n",
      "P04147\n"
     ]
    }
   ],
   "source": [
    "from Bio.Data.IUPACData import protein_letters\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "# the structural window params\n",
    "num_top_contacts = 5\n",
    "\n",
    "# for num_contacts\n",
    "contact_cutoff_distance = 8\n",
    "long_range_contact_cutoff = 12 # long range contacts are more than x residues apart\n",
    "\n",
    "# this is for tr rosetta\n",
    "lookup_table = dict()\n",
    "distance_threshold = 8\n",
    "map_to_dist = np.vectorize(lambda x : distogram_bins_map[x])\n",
    "                           \n",
    "# create a lookup table with the features for faster access\n",
    "for uniprot_id in set(effect_df.uniprot_id):\n",
    "    print(uniprot_id)\n",
    "    lookup_table[uniprot_id] = dict()\n",
    " \n",
    "    # netsurf\n",
    "    netsurf_out = np.load('../processing/gray2018/netsurfp2/'+uniprot_id+'_netsurf.npz')\n",
    "    to_concat = []\n",
    "    for key in netsurf_out:\n",
    "        if key in ['rsa',\n",
    "                   'asa',\n",
    "                   'phi',\n",
    "                   'psi',\n",
    "                   'disorder',\n",
    "                   'q3',\n",
    "                   #'q8',\n",
    "                  ]:\n",
    "            item = netsurf_out[key].squeeze(axis=0)\n",
    "            to_concat.append(item)\n",
    "        else:\n",
    "            continue\n",
    "    lookup_table[uniprot_id]['netsurf_vec'] = np.concatenate(to_concat, axis=1)\n",
    "\n",
    "    # hmmer pssm\n",
    "    lookup_table[uniprot_id]['hmm_pssm'] = joblib.load(\n",
    "        '../processing/gray2018/hmmer/' + uniprot_id + '.hmm_pssm.joblib.xz')['pssm']\n",
    "    lookup_table[uniprot_id]['hmm_residues'] = joblib.load(\n",
    "        '../processing/gray2018/hmmer/' + uniprot_id + '.hmm_pssm.joblib.xz')['colnames']\n",
    "    \n",
    "    # ev couplings\n",
    "    lookup_table[uniprot_id]['ev_couplings_df'] = pd.read_csv('/home/saul/master_thesis_work/processing/gray2018/ev_couplings/'+uniprot_id+'_single_mutant_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "trained-copying",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b333ac726e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mev_pos_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_pos_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev_pos_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maa1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mev_mut_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mev_pos_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mev_pos_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maa2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mev_mut_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoder for aa1 and aa2\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(np.array(list(protein_letters + 'X-')).reshape(-1,1))\n",
    "\n",
    "x_additional = []\n",
    "for _, row in effect_df.iterrows():\n",
    "    if row.dms_id == 'hsp90':\n",
    "        position_cleaned = row.position\n",
    "        ev_prot_df = lookup_table[row.uniprot_id]['ev_couplings_df']\n",
    "        ev_pos_df = ev_prot_df[ev_prot_df.pos == position_cleaned]\n",
    "        print(row.position, position_cleaned, row.aa1, set(ev_pos_df.wt))\n",
    "    else:\n",
    "        position_cleaned = row.position\n",
    "    # mutation likelyhood\n",
    "    aa1_1hot = enc.transform(np.expand_dims(row.aa1, axis = (0,1))).flatten()\n",
    "    aa2_1hot = enc.transform(np.expand_dims(row.aa2, axis = (0,1))).flatten()\n",
    "    mut_pos_netsurf = lookup_table[row.uniprot_id]['netsurf_vec'][position_cleaned - 1].flatten()\n",
    "    # hmmer pssm\n",
    "    mut_pos_pssm = lookup_table[row.uniprot_id]['hmm_pssm'][position_cleaned - 1].flatten()\n",
    "    pssm_colnames = lookup_table[row.uniprot_id]['hmm_residues']\n",
    "    aa1_pssm = mut_pos_pssm[pssm_colnames.index(row.aa1)].flatten()\n",
    "    aa2_pssm = mut_pos_pssm[pssm_colnames.index(row.aa2)].flatten()\n",
    "    delta_pssm = aa1_pssm - aa2_pssm\n",
    "    # ev couplings\n",
    "    ev_prot_df = lookup_table[row.uniprot_id]['ev_couplings_df']\n",
    "    ev_pos_df = ev_prot_df[ev_prot_df.pos == position_cleaned]\n",
    "    if not ev_pos_df.empty:\n",
    "        assert len(set(ev_pos_df.wt)) == 1\n",
    "        assert set(ev_pos_df.wt).pop() == row.aa1\n",
    "        ev_mut_df = ev_pos_df[ev_pos_df.subs == row.aa2]\n",
    "        if not ev_mut_df.empty:\n",
    "            assert len(ev_mut_df) == 1\n",
    "            ev_independent = np.array(ev_mut_df.prediction_independent)\n",
    "            ev_epistatic = np.array(ev_mut_df.prediction_epistatic)\n",
    "        else:\n",
    "            assert row.aa1 == row.aa2\n",
    "            ev_independent = np.array([0])\n",
    "            ev_epistatic = np.array([0])\n",
    "    else:\n",
    "        ev_independent = np.array([np.nan])\n",
    "        ev_epistatic = np.array([np.nan])\n",
    "    assert ev_independent.shape == (1,)\n",
    "    assert ev_epistatic.shape == (1,)\n",
    "    # compile the features\n",
    "    curr_row = np.concatenate([aa1_1hot,\n",
    "                               aa2_1hot,\n",
    "                               mut_pos_netsurf,\n",
    "                               aa1_pssm,\n",
    "                               aa2_pssm,\n",
    "                               delta_pssm,\n",
    "                               mut_pos_pssm,\n",
    "                               ev_independent,\n",
    "                               ev_epistatic,\n",
    "                              ])\n",
    "    x_additional.append(curr_row)\n",
    "\n",
    "x_additional = np.array(x_additional)\n",
    "x = np.concatenate([x_original, x_additional.reshape(x_additional.shape[0], -1)], axis=1)\n",
    "x_additional.shape, x.shape, x_original.shape, effect_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-light",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indie-alexandria",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "def lopo_plots_and_correlation(x_raw, df_raw, target, xbg_params, num_rounds=250):\n",
    "    lopo = LeaveOneGroupOut()\n",
    "    query_encoding = {id_str:list(set(df_raw.dms_id)).index(id_str) for id_str in set(df_raw.dms_id)}\n",
    "    query_ids_raw = np.array(df_raw.dms_id.apply(lambda x : query_encoding[x])) # xgboost requires qid to be numerical\n",
    "    idx_sort = np.argsort(query_ids_raw) # xgboost requires qid to be sorted\n",
    "    query_ids = query_ids_raw[idx_sort]\n",
    "    df = df_raw.iloc[idx_sort]\n",
    "    x = x_raw[idx_sort]\n",
    "    y = np.array(df[target])\n",
    "    d_all = xgb.DMatrix(data=x, label=y)\n",
    "    is_wt = np.array(df.aa1 == df.aa2)\n",
    "    for train, val in lopo.split(x, groups=df.protein):\n",
    "        curr_protein_tested = list(set(df.protein.iloc[val]))\n",
    "        print('Protein:', curr_protein_tested[0])\n",
    "        d_train = xgb.DMatrix(data=x[train], label=y[train], qid=query_ids[train])\n",
    "        y_pred = xgb.train(xgb_params, d_train, num_boost_round=num_rounds).predict(d_all)\n",
    "        # for the train I need to evaluate any dataset independently, ranks among mutations in different proteins are not meaningful\n",
    "        train_spearman_list = []\n",
    "        for dataset in list(set(df.dms_id.iloc[train])):\n",
    "            bool_to_consider = np.array(df.dms_id == dataset)\n",
    "            train_spearman_list.append(stats.spearmanr(y_pred[bool_to_consider], y[bool_to_consider])[0])\n",
    "        train_spearman = np.average(train_spearman_list)\n",
    "        for dataset in list(set(df.dms_id.iloc[val])):\n",
    "            bool_to_consider = np.array(df.dms_id == dataset)\n",
    "            print('Dataset:', dataset)\n",
    "            curr_uniprot_tested = list(set(df.uniprot_id.iloc[bool_to_consider]))\n",
    "            assert len(curr_protein_tested) == 1\n",
    "            print('Uniprot ID:', curr_uniprot_tested)\n",
    "            val_spearman = stats.spearmanr(y_pred[bool_to_consider], y[bool_to_consider])[0]\n",
    "            print('val spearman:', val_spearman, 'train spearman:', train_spearman)\n",
    "            plt.close()\n",
    "            sns.scatterplot(x=y[bool_to_consider & ~is_wt],\n",
    "                            y=y_pred[bool_to_consider & ~is_wt],\n",
    "                            marker='x')\n",
    "            sns.scatterplot(x=y[bool_to_consider & is_wt],\n",
    "                            y=y_pred[bool_to_consider & is_wt],\n",
    "                            s=100,\n",
    "                            alpha=0.5)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-poverty",
   "metadata": {},
   "source": [
    "## Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "small-cookie",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3d4b8a4f2ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m              }\n\u001b[1;32m     13\u001b[0m \u001b[0mnum_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlopo_plots_and_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_additional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffect_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reported_fitness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-4a86c74f5c05>\u001b[0m in \u001b[0;36mlopo_plots_and_correlation\u001b[0;34m(x_raw, df_raw, target, xbg_params, num_rounds)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquery_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_ids_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_sort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_sort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_sort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0md_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "xgb_params = {'max_depth':6,\n",
    "              'min_child_weight':6,\n",
    "              'subsample':0.2,\n",
    "              'colsample_bytree':0.8,\n",
    "              'gamma':1,\n",
    "              'eta':0.05,\n",
    "              'tree_method':'hist',\n",
    "              'objective':'rank:pairwise',\n",
    "              'nthread':7,\n",
    "              'lambda':1, # L2 regularization\n",
    "              'alpha':0, # L1 regularization\n",
    "             }\n",
    "num_rounds = 50\n",
    "lopo_plots_and_correlation(x_additional, effect_df, 'reported_fitness', xgb_params, num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-newcastle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
